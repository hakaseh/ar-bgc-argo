{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ***Generate*** the analysis-ready dataset of a selected BGC-Argo float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">--- User input begins ---</span>\n",
    "- `wmoid`: the float's WMO\n",
    "- `qc2keep`: value(s) of QC flags to be considered valid (\\['1','2','5','8'\\] is a standard choice, but modify as necessary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmoid = 6903574\n",
    "qc2keep = ['1','2','5','8']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">--- User input ends ---</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage import generic_filter\n",
    "import pvlib\n",
    "import gsw\n",
    "import scipy\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Load the input file (Sprof.nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = str(wmoid)+\"/\"+str(wmoid)+\"_Sprof.nc\"\n",
    "ds = xr.open_dataset(in_file) \n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Read all profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for storing all profiles\n",
    "vars_original = ['TEMP_ADJUSTED',\n",
    "                 'PSAL_ADJUSTED',\n",
    "                 'DOWNWELLING_PAR_ADJUSTED',\n",
    "                 'NITRATE_ADJUSTED',\n",
    "                 'CHLA_ADJUSTED',\n",
    "                 'BBP700_ADJUSTED',\n",
    "                 'DOXY_ADJUSTED',\n",
    "                 'PH_IN_SITU_TOTAL_ADJUSTED',\n",
    "                ]\n",
    "\n",
    "def calc_solar_elevation_vectorized(latitude, longitude, times):\n",
    "    \"\"\"\n",
    "    Calculates solar elevation in a fully vectorized way.\n",
    "    Inputs must be array-like (lists, np.array, pd.Series)\n",
    "    of matching lengths.\n",
    "    \n",
    "    Args:\n",
    "        latitude (array-like): Latitudes\n",
    "        longitude (array-like): Longitudes\n",
    "        times (pd.DatetimeIndex or array-like): Timezone-aware datetimes\n",
    "    \"\"\"\n",
    "    \n",
    "    # This function handles the element-wise calculation\n",
    "    solar_position = pvlib.solarposition.get_solarposition(\n",
    "        times, \n",
    "        latitude, \n",
    "        longitude\n",
    "    )\n",
    "    \n",
    "    solar_elevation = 90 - solar_position['zenith'].values\n",
    "    \n",
    "    # This returns a pandas Series (or a numpy array \n",
    "    # if you pass numpy arrays as input)\n",
    "    return solar_elevation\n",
    "\n",
    "# Determine whether to use PRES or PRES_ADJUSTED\n",
    "# use PRES if all values are NaN for PRES_ADJUSTED\n",
    "if np.isnan(ds['PRES_ADJUSTED'].values).all():\n",
    "    name_pres = \"PRES\"\n",
    "# use PRES_ADJUSTED if non-NaN values exist\n",
    "else:\n",
    "    name_pres = \"PRES_ADJUSTED\"\n",
    "\n",
    "# Function to filter the data based on QC flag values\n",
    "# Two xarrays are returned: the values and the depths\n",
    "def ds_valid(name_in,qc_valid):\n",
    "    qc_mask = ds[name_in+\"_QC\"].astype(str).isin(qc_valid)\n",
    "    return ds[name_in].where(qc_mask,other=np.nan),ds[name_pres].where(qc_mask,other=np.nan)\n",
    "\n",
    "# daytime_valid is boolean for daytime profiles; \n",
    "# mask_qc5 is boolean for CHLA_ADJUSTED with at least 1 data with QC = 5 for each profile\n",
    "if \"CHLA_ADJUSTED\" in ds.data_vars and ds_valid(\"CHLA_ADJUSTED\",qc2keep)[0].notnull().any():\n",
    "    daytime_valid = calc_solar_elevation_vectorized(ds['LATITUDE'].values,ds['LONGITUDE'].values,ds['JULD'].values) > 0 # True if sun is above horizon (daytime)\n",
    "    ds_chla_qc5 = ds_valid(\"CHLA_ADJUSTED\",\"5\")[0]\n",
    "    mask_qc5 = ds_chla_qc5.notnull().any(dim=\"N_LEVELS\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Visualize the sampling frequency in time (time difference between consecutive profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,2))\n",
    "spacing_days = ds[\"JULD\"].diff(dim=\"N_PROF\") / np.timedelta64(1, 'D')\n",
    "spacing_days.plot(marker=\"|\",ls=\"None\")\n",
    "plt.title('Time difference between two consecutive profiles (days)')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Visualize the raw data to understand the spatial and temporal coverage of the data\n",
    "- \"NO DATA\" means the variable does not exist.\n",
    "- \"NO VALID DATA\" means the variable exits but all values are NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(vars_original)): # loop over variables\n",
    "    try:\n",
    "        vars_original[j] in ds.data_vars\n",
    "        if ds[vars_original[j]].notnull().any(): # if finite values exit\n",
    "            fig = plt.figure()\n",
    "            mmin,mmax = ds[vars_original[j]].min(),ds[vars_original[j]].max()\n",
    "            ds[vars_original[j]].plot(x=\"N_PROF\",cmap=\"plasma\",vmin=mmin,vmax=mmax)\n",
    "            plt.title(\"[RAW] min: \"+str(mmin.values)+\", max: \"+str(mmax.values))            \n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close(fig)\n",
    "        else:\n",
    "          print('NO VALID DATA for '+vars_original[j])  \n",
    "    except:\n",
    "        print('NO DATA for '+vars_original[j])\n",
    "# PRESSURE\n",
    "fig = plt.figure()\n",
    "mmin,mmax = ds[name_pres].min(),ds[name_pres].max()\n",
    "ds[name_pres].plot(x=\"N_PROF\",cmap=\"plasma\",vmin=mmin,vmax=mmax)\n",
    "plt.title(\"[RAW] min: \"+str(mmin.values)+\", max: \"+str(mmax.values))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Do the same as above, but excluding bad data (retain only QC flags = 1,2,5,8)\n",
    "- \"NO GOOD DATA\" means the variable exits but all values are NaNs before/after applying QC.\n",
    "- QC flags for pressure data are not applied but displayed here for interest (1 or nan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(vars_original)): # loop over variables\n",
    "    try:\n",
    "        good_ds,good_pres = ds_valid(vars_original[j],qc2keep)\n",
    "        if good_ds.notnull().any(): # if finite values exit\n",
    "            fig = plt.figure()\n",
    "            mmin,mmax = good_ds.min(),good_ds.max()\n",
    "            good_ds.plot(x=\"N_PROF\",cmap=\"plasma\",vmin=mmin,vmax=mmax)\n",
    "            plt.title(\"[QC] min: \"+str(mmin.values)+\", max: \"+str(mmax.values))\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close(fig)\n",
    "        else:\n",
    "            print('NO GOOD DATA for '+vars_original[j])\n",
    "    except KeyError:\n",
    "        print('NO DATA for '+vars_original[j])\n",
    "# PRESSURE\n",
    "good_ds,good_pres = ds_valid(name_pres,'1')\n",
    "fig = plt.figure()\n",
    "mmin,mmax = good_ds.min(),good_ds.max()\n",
    "ds[name_pres].plot(x=\"N_PROF\",cmap=\"plasma\",vmin=mmin,vmax=mmax)\n",
    "plt.title(\"[QC] min: \"+str(mmin.values)+\", max: \"+str(mmax.values))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Smoothing for chlorophyll-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    good_ds,good_pres = ds_valid(\"CHLA_ADJUSTED\",qc2keep)\n",
    "    if good_ds.notnull().any(): # if finite values exit\n",
    "        # vertical resolution\n",
    "        pres_res = good_pres.diff(dim=\"N_LEVELS\")\n",
    "        pres_res_med = pres_res.median(dim=\"N_LEVELS\")\n",
    "    \n",
    "        # create a dataset for smoothed profile\n",
    "        ds_smooth = good_ds\n",
    "    \n",
    "        # 1. Create a boolean mask of where valid data currently exists\n",
    "        original_mask = ds_smooth.notnull()\n",
    "        \n",
    "        # loop over profiles\n",
    "        for i in range(pres_res_med.size):\n",
    "            # window size logic (same as yours)\n",
    "            if pres_res_med[i] >= 3:\n",
    "                nsmooth = 5\n",
    "            elif pres_res_med[i] <= 1:\n",
    "                nsmooth = 11\n",
    "            else:\n",
    "                nsmooth = 7\n",
    "            \n",
    "            # 2. Apply Smoothing\n",
    "            # You can keep min_periods=1 here if you want robust smoothing...\n",
    "            smoothed_profile = ds_smooth[i,:].rolling(N_LEVELS=nsmooth, center=True, min_periods=1).median()\n",
    "            \n",
    "            # 3. ...BUT immediately re-apply the original mask\n",
    "            # This forces any index that was originally NaN to stay NaN\n",
    "            ds_smooth[i,:] = smoothed_profile.where(original_mask[i,:])\n",
    "    \n",
    "        # Plot\n",
    "        fig = plt.figure()\n",
    "        mmin,mmax = ds_smooth.min(),ds_smooth.max()\n",
    "        ds_smooth.plot(x=\"N_PROF\",cmap=\"plasma\",vmin=mmin,vmax=mmax)\n",
    "        plt.title(\"[SMOOTH] min: \"+str(mmin.values)+\", max: \"+str(mmax.values))\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        print('NO GOOD DATA for CHLA_ADJUSTED')\n",
    "except KeyError:\n",
    "    print('NO DATA for CHLA_ADJUSTED')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### partitioning of bbp700\n",
    "- method based on Briggs et al. 2020 (page 3 of their SI), but simplified (simply 11-point min and max filters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    good_ds,good_pres = ds_valid(\"BBP700_ADJUSTED\",qc2keep)\n",
    "    if good_ds.notnull().any(): # if finite values exit\n",
    "\n",
    "        # Get valid data\n",
    "        ds_bbp = good_ds\n",
    "        \n",
    "        # --- METHOD 1: STRICT MASKING START ---\n",
    "        \n",
    "        # 1. Capture the original data footprint (Where is data valid?)\n",
    "        original_mask = ds_bbp.notnull()\n",
    "        \n",
    "        # 2. Apply Minimum Filter\n",
    "        # We keep min_periods=1 to handle edges robustly, knowing we will mask later\n",
    "        min_filtered = ds_bbp.rolling(N_LEVELS=11, center=True, min_periods=1).min()\n",
    "        \n",
    "        # 3. Apply Maximum Filter on the result\n",
    "        ds_bbps_unmasked = min_filtered.rolling(N_LEVELS=11, center=True, min_periods=1).max()\n",
    "        \n",
    "        # 4. Re-apply the mask to the result\n",
    "        # This forces the \"Small\" fraction to be NaN wherever the original data was NaN\n",
    "        ds_bbps = ds_bbps_unmasked.where(original_mask)\n",
    "        \n",
    "        # --- METHOD 1 END ---\n",
    "    \n",
    "        # 5. Calculate the \"Large\" fraction (Total - Small)\n",
    "        ds_bbpl = ds_bbp - ds_bbps\n",
    "    \n",
    "        # Plot\n",
    "        fig = plt.figure()\n",
    "        ds_bbp.plot(x=\"N_PROF\",cmap=\"plasma\",robust=True)\n",
    "        mmin,mmax = ds_bbp.min(),ds_bbp.max()\n",
    "        plt.title(\"[BBP700] min: \"+str(mmin.values)+\", max: \"+str(mmax.values))\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close(fig)    \n",
    "        \n",
    "        fig = plt.figure()\n",
    "        ds_bbps.plot(x=\"N_PROF\",cmap=\"plasma\",robust=True)\n",
    "        mmin,mmax = ds_bbps.min(),ds_bbps.max()\n",
    "        plt.title(\"[BBP700SM] min: \"+str(mmin.values)+\", max: \"+str(mmax.values))\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close(fig)    \n",
    "        \n",
    "        fig = plt.figure()\n",
    "        ds_bbpl.plot(x=\"N_PROF\",cmap=\"plasma\",robust=True)\n",
    "        mmin,mmax = ds_bbpl.min(),ds_bbpl.max()\n",
    "        plt.title(\"[BBP700LG] min: \"+str(mmin.values)+\", max: \"+str(mmax.values))\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close(fig)    \n",
    "    \n",
    "    else:\n",
    "        print('NO GOOD DATA for BBP700_ADJUSTED')\n",
    "except KeyError:\n",
    "    print('NO DATA for BBP700_ADJUSTED')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">--- User input begins ---</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_res  = 5.0 # vertical resolution used for interpolation (in dbar)\n",
    "int_dep0 = 1.0 # the shallowest depth\n",
    "int_dep1 = 1000.0 # the deepest depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">--- User input ends ---</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "\n",
    "### Initialization for interpolation\n",
    "- date is sorted for 2d interpolation\n",
    "- Set negatives to zeros for all variables other than temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array for output pressure levels\n",
    "pres_int = np.arange(int_dep0, int_dep1, int_res) \n",
    "\n",
    "# initialize the interpolated dataset\n",
    "ds_int = xr.Dataset()\n",
    "# longitude\n",
    "ds_int['LONGITUDE'] = xr.DataArray(\n",
    "    ds[\"LONGITUDE\"].values,\n",
    "    coords={'time': ('time', ds[\"JULD\"].values)},\n",
    "    dims=['time'],attrs=ds['LONGITUDE'].attrs\n",
    ")\n",
    "# latitude\n",
    "ds_int['LATITUDE'] = xr.DataArray(\n",
    "    ds[\"LATITUDE\"].values,\n",
    "    coords={'time': ('time', ds[\"JULD\"].values)},\n",
    "    dims=['time'],attrs=ds['LATITUDE'].attrs\n",
    ")\n",
    "\n",
    "\n",
    "def interpolate_argo(pres_in, pres_out, data_in):\n",
    "    # Initialize output array with NaNs\n",
    "    data2d = np.full((pres_in.shape[0], len(pres_out)), np.nan)\n",
    "\n",
    "    # loop over profiles\n",
    "    for i in range(pres_in.shape[0]):\n",
    "        # 1. Clean the data: Filter out NaNs\n",
    "        valid_mask = np.isfinite(pres_in[i,:]) & np.isfinite(data_in[i,:])\n",
    "        \n",
    "        curr_pres = pres_in[i,:][valid_mask]\n",
    "        curr_data = data_in[i,:][valid_mask]\n",
    "\n",
    "        # 2. Check points (Need at least 2)\n",
    "        if len(curr_pres) < 2:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # 4. Create the Akima Interpolator\n",
    "            akima = scipy.interpolate.Akima1DInterpolator(curr_pres, curr_data)\n",
    "            \n",
    "            # 5. Interpolate\n",
    "            interpolated_profile = akima(pres_out)\n",
    "            \n",
    "            # 6. Handle Bounds\n",
    "            # Standard approach: Mask values deeper than the profile's max depth\n",
    "            # (We usually allow the surface to extrapolate slightly if the gap is small, \n",
    "            #  but strictly masking the bottom is good practice)\n",
    "            \n",
    "            # Mask ONLY values deeper than the deepest data point\n",
    "            interpolated_profile[pres_out > curr_pres.max()] = np.nan\n",
    "            \n",
    "            # OPTIONAL: Mask values shallower than the shallowest data point\n",
    "            # If you want to keep surface data even if sensor started at 5m, COMMENT THIS OUT:\n",
    "            interpolated_profile[pres_out < curr_pres.min()] = np.nan\n",
    "            \n",
    "            data2d[i, :] = interpolated_profile\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping profile {i} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    return np.float32(data2d)\n",
    "\n",
    "def create_data_array(vars_in,data_in,pres_in,time_in):\n",
    "    data_array = xr.DataArray(\n",
    "        data_in,\n",
    "        coords={'time': ('time', time_in), 'depth': ('depth', pres_in, {'units': 'dbar'})},\n",
    "        dims=['time', 'depth'],\n",
    "        attrs=ds[vars_in].attrs # copy the input file attributes\n",
    "            )\n",
    "    return data_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Interpolate and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate and create xarray data array\n",
    "for j in range(len(vars_original)):\n",
    "    try:\n",
    "        good_ds,good_pres = ds_valid(vars_original[j],qc2keep)\n",
    "        if good_ds.notnull().any(): # if finite values exit\n",
    "            # If the minimum value is negative, offset the profile by that value.\n",
    "            if vars_original[j] == \"CHLA_ADJUSTED\":\n",
    "                good_ds = ds_smooth\n",
    "            elif vars_original[j] == \"BBP700_ADJUSTED\":\n",
    "                good_ds = ds_bbp\n",
    "            data_int = interpolate_argo(good_pres.values,pres_int,good_ds.values)\n",
    "            if vars_original[j] in [\"NITRATE_ADJUSTED\", \"CHLA_ADJUSTED\", \"BBP700_ADJUSTED\"]:\n",
    "                data_int = data_int - np.nanmin(data_int,axis=0).clip(max=0)\n",
    "            ds_int[vars_original[j]+'_AR'] = create_data_array(vars_original[j],data_int,pres_int,ds[\"JULD\"].values)\n",
    "\n",
    "            #Visualization\n",
    "            plt.figure()\n",
    "            ds_int[vars_original[j]+\"_AR\"].plot(x=\"time\",robust=True,cmap=\"plasma\")\n",
    "            mmin,mmax = ds_int[vars_original[j]+\"_AR\"].min(),ds_int[vars_original[j]+\"_AR\"].max()\n",
    "            plt.title(\"[AR] min: \"+str(mmin.values)+\", max: \"+str(mmax.values))\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()            \n",
    "            \n",
    "            # add small (SM) and large (LG) fractions of BBP700_ADJUSTED\n",
    "            if vars_original[j] == \"BBP700_ADJUSTED\":\n",
    "                # SMall\n",
    "                good_ds = ds_bbps\n",
    "                data_int = interpolate_argo(good_pres.values,pres_int,good_ds.values)\n",
    "                data_int = data_int - np.nanmin(data_int,axis=0).clip(max=0)\n",
    "                ds_int[\"BBP700SM_ADJUSTED_AR\"] = create_data_array(\"BBP700_ADJUSTED\",data_int,pres_int,ds[\"JULD\"].values)\n",
    "                # LarGe\n",
    "                good_ds = ds_bbpl\n",
    "                data_int = interpolate_argo(good_pres.values,pres_int,good_ds.values)\n",
    "                data_int = data_int - np.nanmin(data_int,axis=0).clip(max=0)\n",
    "                ds_int[\"BBP700LG_ADJUSTED_AR\"] = create_data_array(\"BBP700_ADJUSTED\",data_int,pres_int,ds[\"JULD\"].values)\n",
    "\n",
    "                plt.figure()\n",
    "                ds_int[\"BBP700SM_ADJUSTED_AR\"].plot(x=\"time\",robust=True,cmap=\"plasma\")\n",
    "                mmin,mmax = ds_int[\"BBP700SM_ADJUSTED_AR\"].min(),ds_int[vars_original[j]+\"_AR\"].max()\n",
    "                plt.title(\"[AR] min: \"+str(mmin.values)+\", max: \"+str(mmax.values))\n",
    "                plt.gca().invert_yaxis()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                plt.close()    \n",
    "                \n",
    "                plt.figure()\n",
    "                ds_int[\"BBP700LG_ADJUSTED_AR\"].plot(x=\"time\",robust=True,cmap=\"plasma\")\n",
    "                mmin,mmax = ds_int[\"BBP700LG_ADJUSTED_AR\"].min(),ds_int[vars_original[j]+\"_AR\"].max()\n",
    "                plt.title(\"[AR] min: \"+str(mmin.values)+\", max: \"+str(mmax.values))\n",
    "                plt.gca().invert_yaxis()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                plt.close()              \n",
    "        else:\n",
    "            print('NO GOOD DATA for '+vars_original[j])\n",
    "    except KeyError:\n",
    "        print('NO DATA for '+vars_original[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Extra\n",
    "- Non-Photochemical Quenching (NPQ) correction for chlrophyll-a\n",
    "- For all profiles collected during daytime, check whether NPQ correction was done (blue) or not (orange). It is unclear why some daytime profiles were NPQ corrected while others are not for some float, but it is possible that the correction was not done when some variables (e.g. T/S/CHLA) contained bad data.\n",
    "- Mixed layer depth based on the 0.03 kg m-3 threshold relative to the reference density at 10 dbar.\n",
    "- Sigma0, spiciness, and oxygen saturation concentration at 0 dbar calculated using GSW-Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize additional variables as 1D or 2D NaNs\n",
    "\n",
    "ds_int['MLD'] = xr.DataArray(\n",
    "    np.full(ds[\"JULD\"].size, np.nan),\n",
    "    coords={'time': ('time', ds[\"JULD\"].values)},\n",
    "    dims=['time'],\n",
    "    attrs={\n",
    "        'long_name': 'Mixed layer depth based on 0.03 kg m-3 density criterion',\n",
    "        'standard_name': 'mixed_layer_depth',\n",
    "        'units': 'm',\n",
    "        'valid_min': np.float32(0.0),\n",
    "        'valid_max': np.float32(1000.0)\n",
    "    }\n",
    ")\n",
    "\n",
    "ds_int['SIGMA0'] = xr.DataArray(\n",
    "    np.full((ds[\"JULD\"].size, len(pres_int)), np.nan),\n",
    "    coords={'time': ('time', ds[\"JULD\"].values), 'depth': ('depth', pres_int, {'units': 'dbar'})},\n",
    "    dims=['time', 'depth'],\n",
    "    attrs={\n",
    "        'long_name': 'Potential density anomaly (sigma-0)',\n",
    "        'standard_name': 'sea_water_sigma_theta',\n",
    "        'comment': 'Potential density anomaly referenced to 0 dbar. See TEOS-10 for details.',\n",
    "        'units': 'kg/m^3 - 1000',  # or '1' (dimensionless, but often reported as \"kg/m3 - 1000\")\n",
    "        'valid_min': np.float32(20.0),\n",
    "        'valid_max': np.float32(30.0)\n",
    "    }\n",
    ")\n",
    "\n",
    "ds_int['SPICINESS0'] = xr.DataArray(\n",
    "    np.full((ds[\"JULD\"].size, len(pres_int)), np.nan),\n",
    "    coords={'time': ('time', ds[\"JULD\"].values), 'depth': ('depth', pres_int, {'units': 'dbar'})},\n",
    "    dims=['time', 'depth'],\n",
    "    attrs={\n",
    "        'long_name': 'Spiciness referenced to a pressure of 0 dbar',\n",
    "        'standard_name': 'spiciness',\n",
    "        'comment': 'see spiciness0 in https://teos-10.github.io/GSW-Python/gsw_flat.html',\n",
    "        'units': 'kg m-3',\n",
    "        'valid_min': np.float32(-100.0),\n",
    "        'valid_max': np.float32(100.0)\n",
    "    }\n",
    ")\n",
    "\n",
    "ds_int['O2SOL'] = xr.DataArray(\n",
    "    np.full((ds[\"JULD\"].size, len(pres_int)), np.nan),\n",
    "    coords={'time': ('time', ds[\"JULD\"].values), 'depth': ('depth', pres_int, {'units': 'dbar'})},\n",
    "    dims=['time', 'depth'],\n",
    "    attrs={\n",
    "        'long_name': 'Oxygen saturation concentration',\n",
    "        'standard_name': 'moles_of_oxygen_per_unit_mass_in_sea_water',\n",
    "        'comment': 'see O2sol in https://teos-10.github.io/GSW-Python/gsw_flat.html',\n",
    "        'units': 'micromole/kg',\n",
    "        'valid_min': np.float32(-5.0),\n",
    "        'valid_max': np.float32(600.0)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"CHLA_ADJUSTED_AR\" in ds_int.data_vars:\n",
    "    # Initialize NPQ-corrected CHLA_ADJUSTED\n",
    "    chla_npq = ds_int['CHLA_ADJUSTED_AR'].copy() # NPQ corrected\n",
    "else:\n",
    "    chla_npq = False\n",
    "    print(\"NO INTERPORATED DATA available for NPQ correction.\")\n",
    "\n",
    "count_npq = 0 # number of NPQ correction applied here\n",
    "# MLD calculation\n",
    "for i in range(ds[\"JULD\"].size): # loop over profiles        \n",
    "    if np.any(np.isfinite(ds_int['TEMP_ADJUSTED_AR'][i,:])) and np.any(np.isfinite(ds_int['PSAL_ADJUSTED_AR'][i,:])): # if good data exists\n",
    "        # Absolute Salinity\n",
    "        SA = gsw.SA_from_SP(ds_int['PSAL_ADJUSTED_AR'][i,:], pres_int, ds_int[\"LONGITUDE\"][i].values, ds_int[\"LATITUDE\"][i].values)        \n",
    "        # Conservative Temperature\n",
    "        CT = gsw.CT_from_t(SA, ds_int['TEMP_ADJUSTED_AR'][i,:], pres_int)\n",
    "        # Saturation oxygen concentration (umol/kg)\n",
    "        ds_int[\"O2SOL\"][i,:] = gsw.O2sol(SA,CT,pres_int,ds_int[\"LONGITUDE\"][i].values,ds_int[\"LATITUDE\"][i].values)\n",
    "        # Spiciness referenced to a pressure level of 0 dbar, i.e. at surface (kg/m^3)\n",
    "        ds_int[\"SPICINESS0\"][i,:] = gsw.spiciness0(SA,CT)\n",
    "        # Potential Density\n",
    "        ds_int[\"SIGMA0\"][i,:] = gsw.sigma0(SA, CT)\n",
    "        # Obtain sigma0 at 10 dbar based on linear interpolation\n",
    "        sigma0_10 = np.interp(10, pres_int, ds_int[\"SIGMA0\"][i,:])\n",
    "        for j in range(ds_int[\"SIGMA0\"][i,:].size): # loop over samples\n",
    "            if ds_int[\"SIGMA0\"][i,:][j] > sigma0_10 + 0.03:\n",
    "                ds_int[\"MLD\"][i] = pres_int[j]\n",
    "                idx90 = np.argmin(np.abs(pres_int - 0.9*ds_int[\"MLD\"][i].values)) # depth index closest to mld*0.9\n",
    "                if \"CHLA_ADJUSTED_AR\" in ds_int.data_vars:\n",
    "                    if np.any(np.isfinite(ds_int['CHLA_ADJUSTED_AR'][i, :])) and daytime_valid[i] and not mask_qc5[i]: \n",
    "                        chla_npq[i,:idx90+1] = np.nanmax(chla_npq[i,:idx90+1]) # set the upper 90% mld to have uniform chla\n",
    "                        count_npq += 1 # count the number of corrected profiles\n",
    "                break # stop at the first occurrence\n",
    "\n",
    "print('Total number of NPQ-corrected profiles in this step:',count_npq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Visualize additional variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_extra = [\"MLD\",\"SIGMA0\",\"SPICINESS0\",\"O2SOL\"]\n",
    "\n",
    "for i in range(len(vars_extra)):\n",
    "    plt.figure()\n",
    "    mmin,mmax = ds_int[vars_extra[i]].min(),ds_int[vars_extra[i]].max()\n",
    "    if len(ds_int[vars_extra[i]].shape) == 1:\n",
    "        ds_int[vars_extra[i]].plot.scatter()\n",
    "    elif len(ds_int[vars_extra[i]].shape) == 2:\n",
    "        ds_int[vars_extra[i]].plot(x=\"time\",robust=True,cmap=\"plasma\")\n",
    "    plt.title(\"[EXTRA] min: \"+str(mmin.values)+\", max: \"+str(mmax.values))\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "# In addition, plot AOU: Apparent Oxygen Utilization\n",
    "if 'DOXY_ADJUSTED_AR' in ds_int.data_vars:\n",
    "    plt.figure()\n",
    "    data2plot =  ds_int[\"O2SOL\"]-ds_int['DOXY_ADJUSTED_AR']\n",
    "    data2plot.plot(x=\"time\",robust=True,cmap=\"plasma\")\n",
    "    mmin,mmax = data2plot.min(),data2plot.max()    \n",
    "    plt.title(\"[AOU = O2SOL - DOXY] \\n min: \"+str(mmin.values)+\", max: \"+str(mmax.values))\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    plt.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Visualize NPQ correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daytime vs QC5\n",
    "if 'CHLA_ADJUSTED_AR' in ds_int.data_vars:\n",
    "    plt.figure()\n",
    "    for i in range(ds[\"JULD\"].size):\n",
    "        # NPQ correction needed\n",
    "        if not mask_qc5[i] and daytime_valid[i]:\n",
    "            plt.scatter(ds[\"JULD\"][i],1,marker=\".\",c=\"r\")\n",
    "        if mask_qc5[i] and not daytime_valid[i]:\n",
    "            plt.scatter(ds[\"JULD\"][i],-1,marker=\".\",c=\"k\")\n",
    "        if mask_qc5[i] and daytime_valid[i]:    \n",
    "            plt.scatter(ds[\"JULD\"][i],0,marker=\".\",c=\"g\")\n",
    "    plt.scatter([],[],marker=\".\",c=\"r\",label=\"Daytime & QC!=5 (NPQ correction required)\")\n",
    "    plt.scatter([],[],marker=\".\",c=\"g\",label=\"Daytime & QC=5 (NPQ correction already done)\")\n",
    "    plt.scatter([],[],marker=\".\",c=\"k\",label=\"Nighttime & QC=5 (why?)\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.ylim(bottom=-2)\n",
    "    plt.yticks([]) \n",
    "    plt.ylabel(None)    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    if count_npq > 0: # visualize if NPQ correction was applied\n",
    "        plt.figure()\n",
    "        ds_int[\"MLD\"].plot.scatter(marker=\"x\",zorder=2,s=1,c=\"k\")\n",
    "        data2plot = chla_npq-ds_int[\"CHLA_ADJUSTED_AR\"]\n",
    "        data2plot.where(data2plot != 0, other=np.nan).plot(x=\"time\",robust=True,cmap=\"plasma\")\n",
    "        mmin,mmax = data2plot.min(),data2plot.max()\n",
    "        plt.title(\"[NPQ = Corrected - Uncorrected] \\n min: \"+str(mmin.values)+\", max: \"+str(mmax.values))\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.ylim(bottom=ds_int[\"MLD\"].max())\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "else:\n",
    "    print(\"Nothing to show, as CHLA_ADJUSTED_AR does not exit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Saving to netCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If NPQ correction was done, replace the chlorophyll-a array\n",
    "if count_npq > 0:\n",
    "    ds_int['CHLA_ADJUSTED_AR'] = chla_npq\n",
    "\n",
    "# Add metadata\n",
    "ds_int.attrs['title'] = 'Analysis-ready BGC-Argo float time series'\n",
    "ds_int.attrs['institution'] = 'Japan Agency for Marine-Earth Science and Technology (JAMSTEC)'\n",
    "ds_int.attrs['notes'] = 'Reference: Fujishima and Hayashida (2026): Jupyter Notebook for generating analysis-ready BGC-Argo float time series, Journal of Open Source Software'\n",
    "ds_int.attrs['history'] = 'Created on ' + datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "# Save to NetCDF\n",
    "ds_int.to_netcdf(str(wmoid)+'/AR'+str(wmoid)+'.nc')\n",
    "\n",
    "ds_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
