{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Plot and generate (Steps 3 - 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Load the libraries (if error occurs, install them via `pip install xarray` or `conda install xarray` for example for `xarray`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage import generic_filter\n",
    "import pvlib\n",
    "import gsw\n",
    "import scipy\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Specify the float you want to make analysis-ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmoid = 5906503 # float number\n",
    "\n",
    "vernum = 0 # Version of the notebook (generally, no need to change) \n",
    "\n",
    "list_file = glob.glob(f\"{wmoid}/S*{wmoid}_[0-9][0-9][0-9].nc\")\n",
    "print('Total number of profies:',len(list_file))\n",
    "list_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Read all profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for storing all profiles\n",
    "juld = [] # date and time\n",
    "lon = [] # longitude\n",
    "lat = [] # latitude\n",
    "pres = [] # pressure level (depth)\n",
    "vars = ['temp','psal','down','nitr','chla','bbp7','doxy','ph_i'] # shortened names\n",
    "vars_original = ['TEMP','PSAL', # original names\n",
    "                 'DOWNWELLING_PAR_ADJUSTED','NITRATE_ADJUSTED',\n",
    "                 'CHLA_ADJUSTED','BBP700_ADJUSTED',\n",
    "                 'DOXY_ADJUSTED','PH_IN_SITU_TOTAL_ADJUSTED']\n",
    "cmap_vars = ['inferno', 'cividis','viridis','magma','Greens','plasma','Blues','coolwarm'] # colormap for each variable\n",
    "raw = {f\"{var}\": [] for var in vars} # raw data\n",
    "qc = {f\"{var}\": [] for var in vars} # qc flags\n",
    "qc_valid = {f\"{var}\": [] for var in vars} # good data\n",
    "pres_qc_valid = {f\"{var}\": [] for var in vars} # corresponding pres for good data\n",
    "npq5_qc_valid = [] # corresponding qc=5 for good chla data (used for identifying whether npq correction is needed)\n",
    "daytime_valid = []\n",
    "\n",
    "def calc_solar_elevation(latitude, longitude, utc):\n",
    "    \"\"\"緯度, 経度, UTC時間 から太陽光角度を計算\"\"\"\n",
    "    loc = pvlib.location.Location(latitude, longitude)\n",
    "    solar_position = loc.get_solarposition(utc)\n",
    "    solar_zenith = solar_position['zenith'].values[0]  # 太陽天頂角\n",
    "    solar_elevation = 90 - solar_zenith  # 太陽高度角\n",
    "    return solar_elevation\n",
    "\n",
    "for i in range(len(list_file)): # loop over profiles\n",
    "    ds = xr.open_dataset(list_file[i]) # open the netCDF file for each profile\n",
    "    juld.append(ds['JULD'][0].values)\n",
    "    lon.append(ds['LONGITUDE'][0].values)\n",
    "    lat.append(ds['LATITUDE'][0].values)\n",
    "    pres.append(ds['PRES'][0,:].values)\n",
    "    daytime_valid.append(calc_solar_elevation(lat[-1],lon[-1],juld[-1]) > 0) # True if sun is above horizon (daytime)\n",
    "    for j in range(len(vars)): # loop over variables\n",
    "        if vars_original[j] in ds.data_vars:\n",
    "            raw[vars[j]].append(ds[vars_original[j]][0,:].values) # store raw profiles\n",
    "            qc[vars[j]].append(ds[vars_original[j]+'_QC'][0,:].values.astype(str)) # store qc flags\n",
    "            if vars_original[j] == 'CHLA_ADJUSTED': # include the qc flag of 5 (NPQ)\n",
    "                qc_valid[vars[j]].append(raw[vars[j]][-1][np.isin(qc[vars[j]][-1],['1','2','5','8'])]) # store qc masks\n",
    "                pres_qc_valid[vars[j]].append(pres[-1][np.isin(qc[vars[j]][-1],['1','2','5','8'])])\n",
    "                npq5_qc_valid.append(np.any(np.isin(qc[vars[j]][-1],['5']))) # set to True if the profile contains QC of 5 (NPQ corrected)\n",
    "            else:\n",
    "                qc_valid[vars[j]].append(raw[vars[j]][-1][np.isin(qc[vars[j]][-1],['1','2','8'])]) # store qc masks\n",
    "                pres_qc_valid[vars[j]].append(pres[-1][np.isin(qc[vars[j]][-1],['1','2','8'])]) # store qc masks\n",
    "\n",
    "# sort the data in the chronological order\n",
    "ind_sorted = np.argsort(juld)\n",
    "juld = [juld[i] for i in ind_sorted]\n",
    "lon = [lon[i] for i in ind_sorted]\n",
    "lat = [lat[i] for i in ind_sorted]\n",
    "pres = [pres[i] for i in ind_sorted]\n",
    "daytime_valid = [daytime_valid[i] for i in ind_sorted]\n",
    "for j in range(len(vars)): # loop over variables\n",
    "    if vars_original[j] in ds.data_vars:\n",
    "        raw[vars[j]] = [raw[vars[j]][i] for i in ind_sorted]\n",
    "        qc[vars[j]] = [qc[vars[j]][i] for i in ind_sorted]\n",
    "        qc_valid[vars[j]] = [qc_valid[vars[j]][i] for i in ind_sorted]\n",
    "        pres_qc_valid[vars[j]] = [pres_qc_valid[vars[j]][i] for i in ind_sorted]\n",
    "        if vars_original == 'CHLA_ADJUSTED':\n",
    "            npq5_qc_valid[vars[j]] = [npq5_qc_valid[vars[j]][i] for i in ind_sorted]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Plot the raw data to understand the data coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raw(vari_in,pres_in,juld_in,vari_name_in,cmap_in):\n",
    "    for i in range(len(vari_in)): # loop over profiles\n",
    "        if np.any(np.isfinite(vari_in[i])): # ignore the profile with all NaNs\n",
    "            plt.scatter(np.full(len(pres_in[i]),juld_in[i]),pres_in[i],c=vari_in[i],s=0.1,cmap=cmap_in,\n",
    "                        vmin=np.nanmin(np.concatenate(vari_in)),\n",
    "                        vmax=np.nanmax(np.concatenate(vari_in))\n",
    "                       )\n",
    "    plt.gca().invert_yaxis()\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label(vari_name_in)\n",
    "    vari_ptile = [np.nanpercentile(np.concatenate(vari_in),25),\n",
    "                  np.nanpercentile(np.concatenate(vari_in),50),\n",
    "                  np.nanpercentile(np.concatenate(vari_in),75)\n",
    "                 ]\n",
    "    cbar.ax.hlines(vari_ptile,xmin=0,xmax=1,color='r') # draw percentiles\n",
    "    plt.gcf().autofmt_xdate() # automatically format date\n",
    "    plt.xlim(np.min(juld),np.max(juld)) # align the date range across all variables\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "for j in range(len(vars)): # loop over variables\n",
    "    plt.subplot(3,3,j+1)\n",
    "    if len(raw[vars[j]]) and np.any(np.isfinite(np.concatenate(raw[vars[j]]))): # if finite values exit\n",
    "        plot_raw(raw[vars[j]],pres,juld,vars_original[j],cmap_vars[j])\n",
    "    else:\n",
    "        plt.text(0.1,0.5,'NO DATA for \\n'+vars_original[j])\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### If chlorophyll-a exists, show the status of NPQ correction\n",
    "- For all profiles collected during daytime, check whether NPQ correction was done (black) or not (red)\n",
    "- QC = 5 means NPQ corrected profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(raw['chla']):\n",
    "    for i in range(len(juld)):\n",
    "        if daytime_valid[i]:\n",
    "            if npq5_qc_valid[i]:\n",
    "                plt.scatter(juld[i],0,marker='|',color='k')\n",
    "            else:\n",
    "                plt.scatter(juld[i],0,marker='|',color='r')\n",
    "    plt.scatter([],[],marker='|',color='k',label=r\"QC = 5\")\n",
    "    plt.scatter([],[],marker='|',color='r',label=r\"QC $\\ne$ 5\")\n",
    "    plt.legend()\n",
    "    plt.title('Daytime profiles')\n",
    "    plt.gcf().autofmt_xdate() # automatically format date\n",
    "    plt.xlim(np.min(juld),np.max(juld)) # align the date range across all variables\n",
    "    plt.gca().axes.get_yaxis().set_visible(False)\n",
    "else:\n",
    "    print('CHLA_ADJUSTED is empty!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Plotting the good data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "for j in range(len(vars)): # loop over variables\n",
    "    plt.subplot(3,3,j+1)\n",
    "    if len(qc_valid[vars[j]]) and np.any(np.isfinite(np.concatenate(qc_valid[vars[j]]))): # if finite values exit\n",
    "        plot_raw(qc_valid[vars[j]],pres_qc_valid[vars[j]],juld,vars_original[j],cmap_vars[j])\n",
    "    else:\n",
    "        plt.text(0.1,0.5,'NO DATA for \\n'+vars_original[j])\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN を無視する中央値フィルタ関数\n",
    "def nanmedian_filter(values):\n",
    "    valid_values = values[~np.isnan(values)]  # NaN を除去\n",
    "    return np.median(valid_values) if len(valid_values) > 0 else np.nan  # 有効値があれば中央値、なければ NaN\n",
    "\n",
    "# define lists\n",
    "smooth = {f\"{var}\": [] for var in vars} # smoothed data\n",
    "pres_res = {f\"{var}\": [] for var in vars} # vertical resolution\n",
    "pres_mid = {f\"{var}\": [] for var in vars} # midpoint depth at which vertical resolutions are defined\n",
    "\n",
    "# Compute and assign smoothed values\n",
    "for j in range(len(vars)): # loop over variables\n",
    "    for i in range(len(qc_valid[vars[j]])): # loop over profiles\n",
    "        pres_res[vars[j]].append(np.diff(pres_qc_valid[vars[j]][i])) # 深度の解像度を計算\n",
    "        pres_mid[vars[j]].append((pres_qc_valid[vars[j]][i][:-1] + pres_qc_valid[vars[j]][i][1:]) / 2) # 深度の解像度を計算\n",
    "        pres_res_med = np.median(pres_res[vars[j]][-1])\n",
    "        # 窓サイズの決定\n",
    "        if pres_res_med >= 3:\n",
    "            nsmooth = 5\n",
    "        elif pres_res_med <= 1:\n",
    "            nsmooth = 11\n",
    "        else:\n",
    "            nsmooth = 7\n",
    "        # 中央値フィルタを適用\n",
    "        smooth[vars[j]].append(generic_filter(\n",
    "            qc_valid[vars[j]][i],nanmedian_filter,size=nsmooth,mode='nearest')\n",
    "                              )\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,10))\n",
    "for j in range(len(vars)): # loop over variables\n",
    "    plt.subplot(3,3,j+1)\n",
    "    if len(smooth[vars[j]]) and np.any(np.isfinite(np.concatenate(smooth[vars[j]]))): # if finite values exit\n",
    "        plot_raw(smooth[vars[j]],pres_qc_valid[vars[j]],juld,vars_original[j],cmap_vars[j])\n",
    "    else:\n",
    "        plt.text(0.1,0.5,'NO DATA for \\n'+vars_original[j])\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Plot the vertical resolution information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_res(pres_res_in,pres_mid_in,vari_name_in):\n",
    "    for i in range(len(pres_res_in)):\n",
    "        if np.any(np.isfinite(pres_res_in[i])): # ignore the profile with all NaNs\n",
    "            plt.scatter(pres_res_in[i],pres_mid_in[i],color='k',s=0.1,alpha=0.1)\n",
    "    plt.title(vari_name_in)\n",
    "    plt.xlim(0,np.nanmax(np.concatenate(pres_res_in)))\n",
    "    plt.ylim(0,np.nanmax(np.concatenate(pres_mid_in)))\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('Resolution (dbar)')\n",
    "    plt.ylabel('Depth (dbar)')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,10))\n",
    "for j in range(len(vars)): # loop over variables\n",
    "    plt.subplot(3,3,j+1)\n",
    "    if len(pres_res[vars[j]]) and np.any(np.isfinite(np.concatenate(pres_res[vars[j]]))): # if finite values exit\n",
    "        plot_res(pres_res[vars[j]],pres_mid[vars[j]],vars_original[j])\n",
    "    else:\n",
    "        plt.text(0.1,0.5,'NO DATA for \\n'+vars_original[j])\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Interpolation (Please specify the resolution and the depth for interpolation)\n",
    "- date is sorted for 2d interpolation\n",
    "- Set negatives to zeros for all variables other than temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_res = 5 # resolution used for interpolation (dbar, which can be approximated as meter)\n",
    "int_dep0 = 1.0 # the shallowest depth\n",
    "int_dep1 = 1000.0 # the deepest depth\n",
    "\n",
    "pres_int = np.arange(int_dep0,int_dep1,int_res) # depth grid for interpolation\n",
    "data_int = {f\"{var}\": [] for var in vars} # interpolated data\n",
    "\n",
    "def interpolate_argo(pres_in,pres_out,data_in,date,vari_name_in):\n",
    "    data2d = np.full((len(date),len(pres_out)), np.nan) # create 2d array filled with NaNs\n",
    "    for i in range(len(pres_in)):\n",
    "        if np.any(np.isfinite(pres_in[i])): # ignore the profile with all NaNs\n",
    "            f = scipy.interpolate.interp1d(x=pres_in[i],y=data_in[i],\n",
    "                                           kind='linear',\n",
    "                                           bounds_error=False,  # 範囲外は補間せずにfill_valueを適用\n",
    "                                           fill_value=np.nan    # 範囲外のデータはNaNに設定\n",
    "                                          )\n",
    "            data2d[i,:] = f(pres_out)\n",
    "    return np.float32(data2d) # single precision is sufficient\n",
    "\n",
    "# function to plot the interpolated data (2d array)\n",
    "def plot_int(vari_in,pres_in,juld_in,vari_name_in,cmap_in):\n",
    "    X, Y = np.meshgrid(juld_in,pres_in, indexing='ij')  # (time, depth)\n",
    "    if np.any(np.isfinite(vari_in)): # check at least one value exits\n",
    "        plt.scatter(X,Y,c=vari_in,s=0.1,vmin=np.nanmin(vari_in),vmax=np.nanmax(vari_in),cmap=cmap_in)\n",
    "    else: # all values are NaNs, which seems weird\n",
    "        print('all interpolated values are NaN? CHECK')\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label(vari_name_in)\n",
    "    vari_ptile = [np.nanpercentile(vari_in,25),\n",
    "                  np.nanpercentile(vari_in,50),\n",
    "                  np.nanpercentile(vari_in,75)\n",
    "                 ]\n",
    "    cbar.ax.hlines(vari_ptile,xmin=0,xmax=1,color='r') # draw percentiles\n",
    "    plt.gcf().autofmt_xdate() # automatically format date\n",
    "    plt.xlim(np.min(juld),np.max(juld)) # align the date range across all variables\n",
    "    plt.ylim(0,np.max(pres_in)) # align the depth range across all variables\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "# Interpolate and plot\n",
    "plt.figure(figsize=(12,10))\n",
    "for j in range(len(vars)): # loop over variables\n",
    "    plt.subplot(3,3,j+1)\n",
    "    if len(smooth[vars[j]]) and np.any(np.isfinite(np.concatenate(smooth[vars[j]]))): # if finite values exit\n",
    "        data_int[vars[j]] = interpolate_argo(pres_qc_valid[vars[j]],pres_int,smooth[vars[j]],juld,vars_original[j])\n",
    "        if vars[j] != 'temp': # if not temperature\n",
    "            data_int[vars[j]][data_int[vars[j]]<0] = 0 # set negatives to zeros\n",
    "        plot_int(data_int[vars[j]],pres_int,juld,vars_original[j],cmap_vars[j])        \n",
    "    else:\n",
    "        plt.text(0.1,0.5,'NO DATA for \\n'+vars_original[j])\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Store as an xarray dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary to hold DataArrays\n",
    "data_vars = {}\n",
    "\n",
    "# Loop to generate DataArrays\n",
    "for i in range(len(vars)):\n",
    "    if len(data_int[vars[i]]): # continue if data exist\n",
    "        data_array = xr.DataArray(\n",
    "            data_int[vars[i]],\n",
    "            coords={\n",
    "                'time': ('time', juld), #, {'units': 'days since 1950-01-01'}),\n",
    "                'depth': ('depth', pres_int, {'units': 'dbar'})  # or 'meters' if it's depth below sea surface\n",
    "            },            \n",
    "            dims=['time', 'depth'],\n",
    "            attrs=ds[vars_original[i]].attrs # copy the input file attributes\n",
    "        )\n",
    "        data_vars[vars_original[i]+'_AR'] = data_array # adding the array to the dataset (AR: Analysis-Ready)\n",
    "    else: # skip if data are empty\n",
    "        print(vars_original[i],'is empty so not adding to the file')\n",
    "\n",
    "# add longitude and latitude as additonal variables in case of potential use\n",
    "data_vars['LONGITUDE'] = xr.DataArray(lon,coords={'time': ('time', juld)},dims=['time'],attrs=ds['LONGITUDE'].attrs)\n",
    "data_vars['LATITUDE'] = xr.DataArray(lat,coords={'time': ('time', juld)},dims=['time'],attrs=ds['LATITUDE'].attrs)\n",
    "\n",
    "# Create Dataset from all variables\n",
    "ds_int = xr.Dataset(data_vars)\n",
    "\n",
    "print(ds_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Extra\n",
    "- Chlorophyll-a: Non-Photochemical Quenching (NPQ) correction.\n",
    "- Mixed layer depth based on the 0.03 kg m-3 threshold relative to the reference density at 10 dbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define empty lists\n",
    "mld = [] # mixed layer depth\n",
    "chla_npq = data_int['chla'].copy() # NPQ corrected\n",
    "count_npq = 0 # number of NPQ correction applied here\n",
    "\n",
    "# MLD calculation\n",
    "if len(data_int['temp']): # if temp data exist:\n",
    "    for i in range(len(data_int['temp'])): # loop over profiles        \n",
    "        if np.all(~np.isnan(data_int['temp'][i,:])) and np.all(~np.isnan(data_int['psal'][i,:])): # if good data exists\n",
    "            # 絶対塩分（Absolute Salinity, SA）の計算\n",
    "            SA = gsw.SA_from_SP(data_int['psal'][i,:], pres_int, lon[i], lat[i])        \n",
    "            # 実効温度（Conservative Temperature, CT）の計算\n",
    "            CT = gsw.CT_from_t(SA, data_int['temp'][i,:], pres_int)\n",
    "            # ポテンシャル密度（Potential Density, σθ）の計算（基準圧力 0 dbar）\n",
    "            sigma0 = gsw.sigma0(SA, CT)  # σθ = 密度 - 1000 (kg/m³)\n",
    "            # Obtain sigma0 at 10 dbar based on linear interpolation 10dbarでの密度を取得\n",
    "            sigma0_10 = np.interp(10, pres_int, sigma0)\n",
    "            for j in range(len(sigma0)): # loop over samples\n",
    "                if sigma0[j] > sigma0_10 + 0.03:\n",
    "                    mld.append(pres_int[j])\n",
    "                    idx90 = np.argmin(np.abs(pres_int - 0.9*mld[-1])) # depth index closest to mld*0.9\n",
    "                    if np.any(~np.isnan(data_int['chla'][i, :])) and daytime_valid[i] and not npq5_qc_valid[i]: # if good data exists, sun is above horizon, and qc != 5 (NPQ correction necessary and possible)\n",
    "                        chla_npq[i,:idx90+1] = np.nanmax(chla_npq[i,:idx90+1]) # set the upper 90% mld to have uniform chla\n",
    "                        count_npq += 1 # count the number of corrected profiles\n",
    "                    break # stop at the first occurrence\n",
    "            else: # if the threshold is never met\n",
    "                mld.append(np.nan) # assign nan because MLD was not found\n",
    "        else:\n",
    "            mld.append(np.nan) # assign nan because no good temp and psal data exit\n",
    "else:\n",
    "    print('Doing nothing as no temp data exist (this should not happen...)')\n",
    "\n",
    "print('Total number of NPQ-corrected profiles in this step:',count_npq)\n",
    "\n",
    "# Add MLD as an additional variable\n",
    "data_vars['MLD'] = xr.DataArray(mld,coords={'time': ('time', juld)},dims=['time'],\n",
    "                                attrs={\n",
    "                                'long_name': 'Mixed layer depth based on 0.03 kg m-3 density criterion',\n",
    "                                'standard_name': 'mixed_layer_depth',\n",
    "                                'units': 'm',\n",
    "                                'valid_min': np.float32(0.0),\n",
    "                                'valid_max': np.float32(1000.0)\n",
    "                                }\n",
    "                               )\n",
    "\n",
    "# Visualize the calculated MLD and the effect of the NPQ correction.\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.subplot(131)\n",
    "plt.scatter(juld,mld,zorder=2,s=1,c='k')\n",
    "plt.scatter([],[],zorder=2,s=1,c='k',label='MLD')\n",
    "plt.legend()\n",
    "if count_npq > 0: # if NPQ correction was done\n",
    "    plot_int(data_int['chla'],pres_int,juld,'(a) CHLA_ADJUSTED','Greens')\n",
    "    plt.ylim(200,0)\n",
    "    plt.subplot(132)\n",
    "    plot_int(chla_npq,pres_int,juld,'(b) CHLA_ADJUSTED_NPQ','Greens')\n",
    "    plt.ylim(200,0)\n",
    "    plt.subplot(133)\n",
    "    diff = chla_npq - data_int['chla']\n",
    "    masked_diff = np.where(diff != 0, diff, np.nan)\n",
    "    plot_int(masked_diff,pres_int,juld,'b minus a','Purples')\n",
    "    plt.ylim(200,0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Saving to netCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If NPQ correction was done, replace the chlorophyll-a array\n",
    "if count_npq > 0:\n",
    "    ds_int['CHLA_ADJUSTED_AR'].values = chla_npq\n",
    "\n",
    "# Add metadata\n",
    "ds_int.attrs['title'] = 'Analysis-ready BGC-Argo dataset'\n",
    "ds_int.attrs['institution'] = 'JAMSTEC Application Laboratory (APL)'\n",
    "ds_int.attrs['notes'] = 'Reference: Hayashida and Fujishima (2025): Jupyter Notebook for generating analysis-ready BGC-Argo datasets, Journal of Open Source Software'\n",
    "ds_int.attrs['history'] = 'Created on ' + datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\") + ' using Version '+str(vernum)\n",
    "\n",
    "# Save to NetCDF\n",
    "ds_int.to_netcdf('AR'+str(wmoid)+'.nc')\n",
    "\n",
    "print(ds_int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
