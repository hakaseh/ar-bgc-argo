{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import netCDF4 as nc4\n",
    "from netCDF4 import Dataset\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import os\n",
    "import math\n",
    "import ruptures as rpt\n",
    "import multiprocessing\n",
    "from pylab import rcParams\n",
    "import glob\n",
    "import gsw\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import patches\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "import scipy\n",
    "from scipy import interpolate\n",
    "from scipy import signal\n",
    "from scipy.interpolate import griddata\n",
    "from scipy import stats\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.util as cutil\n",
    "from cartopy.mpl.ticker import LatitudeFormatter,LongitudeFormatter\n",
    "\n",
    "dirhome = os.environ['HOME']\n",
    "plt.rcParams['font.family'] = 'Times New Roman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be called separately from the first line for some reason (otherwise the above xr.open_dataset code gives an error, dunno why!)\n",
    "import pvlib\n",
    "from pvlib.location import Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "import re\n",
    "dirhome = os.environ['HOME']\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "#Uncomment the following lines related to dask if kernel gets killed.\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client\n",
    "\n",
    "#list of tab colors\n",
    "import matplotlib.colors as mcolors\n",
    "tabcol = list(mcolors.TABLEAU_COLORS)\n",
    "abc = 'abcdefghijklmopqrstuvwxyz'\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "#from chatgpt\n",
    "def extract_before_last_slash(input_string):\n",
    "    last_slash_index = input_string.rfind('/')\n",
    "    if last_slash_index != -1:\n",
    "        return input_string[:last_slash_index]\n",
    "    else:\n",
    "        return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv('https://data-argo.ifremer.fr/argo_synthetic-profile_index.txt',skiprows=8)\n",
    "    print('Accessing the IFREMER server')\n",
    "except:\n",
    "    data = pd.read_csv('https://usgodae.org/ftp/outgoing/argo/argo_synthetic-profile_index.txt',skiprows=8)\n",
    "    print('The IFREMER server is currently not accessible. Accessing the USGODAE server instead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon0 = 120\n",
    "lon1 = 150\n",
    "lat0 = 20\n",
    "lat1 = 50\n",
    "date0 = 20240101\n",
    "date1 = 20241231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the dataset based on lon, lat, and variable (chl). Date has a weird format so extract first then postprocess dates\n",
    "datasub = data[(data['longitude'] >= lon0) & (data['longitude'] <= lon1) &\n",
    "               (data['latitude'] >= lat0) & (data['latitude'] <= lat1) &\n",
    "               (data['date'] >= 0) & \n",
    "               (data['parameters'].str.contains('NITRATE')) & \n",
    "               (data['parameter_data_mode'].str[-1].isin(['A', 'D']))]\n",
    "\n",
    "#add the new variable called 'hour' referring to the hour of sampling\n",
    "#extract the two digits after yyyymmdd corresponding to hh of hhmmss\n",
    "hourstr = [str(num) for num in datasub['date']]\n",
    "hourstr = [num[8:8+2] for num in hourstr]\n",
    "hourint = [int(num) for num in hourstr]\n",
    "datasub['hour'] = hourint\n",
    "\n",
    "#add the new variable called 'time' referring to the hour of sampling\n",
    "#extract the 6 digits after yyyymmdd corresponding to hhmmss\n",
    "#NOTE: time is provided as string to avoid inconsistent digits for hours\n",
    "timestr = [str(num) for num in datasub['date']]\n",
    "timestr = [num[8:8+6] for num in timestr]\n",
    "datasub['time'] = timestr\n",
    "\n",
    "#modify the date format to be yyyymmdd\n",
    "datestr = [str(num) for num in datasub['date']]\n",
    "datestr = [num[0:8] for num in datestr]\n",
    "dateint = [int(num) for num in datestr]\n",
    "datasub['date'] = dateint\n",
    "#create a new column called floatid for lagrangian time series plot\n",
    "tmpfloatid = []\n",
    "for i in range(np.size(datasub['date'])):\n",
    "    tmpfloatid.append(datasub['file'].values[i].split('/')[1])\n",
    "datasub['floatid'] = tmpfloatid\n",
    "#refine the dataset based on the selected period\n",
    "datasub = datasub[(datasub['date'].values >= date0) & (datasub['date'].values <= date1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasub['file'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(datasub.shape[0]):\n",
    "    dir = 'ftp.ifremer.fr/ifremer/argo/dac/'+datasub['file'].values[i]\n",
    "    !curl -L -O {dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## The following is the correction of data.\n",
    "\n",
    "#### Choose the float you use\n",
    "##### For example SD5906510,SR7902116..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Read datas from all profiles from the float you choosed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnum = 'SD5906519'\n",
    "var1 = \"NITRATE\"\n",
    "var2 = \"NITRATE_ADJUSTED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = f\"{fnum}_*.nc\"\n",
    "\n",
    "# ファイルパターンに一致するすべてのファイルを取得\n",
    "file_list = sorted(glob.glob(file_pattern))  # ソートして番号順に処理\n",
    "if not file_list:\n",
    "    raise FileNotFoundError(f\"No files found matching pattern: {file_pattern}\")\n",
    "    \n",
    "# データを格納するリスト\n",
    "concat_plist = []\n",
    "concat_slist = []\n",
    "#concat_var1_list = []\n",
    "concat_var2_list = []\n",
    "qc_list = []\n",
    "lon_list = []\n",
    "lat_list = []\n",
    "time_list = []\n",
    "min_depth_size = None\n",
    "base_date = datetime(1950, 1, 1)\n",
    "\n",
    "# ファイルを順番に読み込む\n",
    "for file_name in file_list:        \n",
    "    # netCDF ファイルを開く\n",
    "    with Dataset(file_name, mode='r') as nc:\n",
    "        if \"NITRATE_ADJUSTED\" not in nc.variables:\n",
    "            print(f\"Variable 'NITRATE_ADJUSTED' not found in {file_name}\")\n",
    "            continue\n",
    "\n",
    "        # QC フラグの取得とチェック\n",
    "        if \"NITRATE_ADJUSTED_QC\" in nc.variables:\n",
    "            qc_flags = nc.variables[\"NITRATE_ADJUSTED_QC\"][:]\n",
    "            qc_flags_str = np.where(qc_flags.mask, ' ', qc_flags.astype(str))  # マスク部分は空白\n",
    "            valid_qc_mask = np.isin(qc_flags_str, ['1', '2', '8'])  # QCが1,2,5のもの\n",
    "        else:\n",
    "            print(f\"NITRATE_ADJUSTED_QC が {file_name} に存在しません。\")\n",
    "            continue\n",
    "\n",
    "        # QC フラグの取得とチェック\n",
    "        if \"PSAL_QC\" in nc.variables:\n",
    "            sqc_flags = nc.variables[\"PSAL_QC\"][:]\n",
    "            sqc_flags_str = np.where(sqc_flags.mask, ' ', sqc_flags.astype(str))  # マスク部分は空白\n",
    "            valid_sqc_mask = np.isin(sqc_flags_str, ['1', '2'])  # QCが1,2のもの\n",
    "        else:\n",
    "            print(f\"PSAL_QC が {file_name} に存在しません。\")\n",
    "            continue\n",
    "        \n",
    "        # 指定された変数のデータを取得\n",
    "        # 時間ステップ\n",
    "        juld = nc.variables[\"JULD\"][:][0]\n",
    "        time = base_date + timedelta(days=juld)\n",
    "\n",
    "        # 緯度・経度データ\n",
    "        lat = nc.variables[\"LATITUDE\"][:][0]\n",
    "        lon = nc.variables[\"LONGITUDE\"][:][0]\n",
    "\n",
    "        # 変数\n",
    "        pdata = nc.variables[\"PRES\"][:, :]\n",
    "        sdata = nc.variables[\"PSAL\"][:, :]\n",
    "\n",
    "        #var1_data = nc.variables[\"NITRATE\"][:, :]\n",
    "        var2_data = nc.variables[\"NITRATE_ADJUSTED\"][:, :]\n",
    "        lon_data = nc.variables[\"LONGITUDE\"][:]\n",
    "        lat_data = nc.variables[\"LATITUDE\"][:]\n",
    "        # mask\n",
    "        pdata = np.where(pdata >= 99999.0, np.nan, pdata)\n",
    "        #var1_data = np.where(var1_data >= 99999.0, np.nan, var1_data)\n",
    "        var2_data = np.where(var2_data >= 99999.0, np.nan, var2_data)\n",
    "\n",
    "        # QC フラグでフィルタリング\n",
    "        var2_data[~valid_qc_mask] = np.nan\n",
    "        sdata[~valid_sqc_mask] = np.nan\n",
    "\n",
    "\n",
    "        # 深度方向のサイズを取得\n",
    "        depth_size = pdata.shape[1]        \n",
    "        # 最初のファイルの場合、最小深度を初期化\n",
    "        if min_depth_size is None:\n",
    "            min_depth_size = depth_size\n",
    "        \n",
    "        # 深度方向のサイズを調整\n",
    "        if depth_size < min_depth_size:\n",
    "            # すでに存在するデータを新しい最小深度に切り詰める\n",
    "            concat_plist = [d[:, :depth_size] for d in concat_plist]\n",
    "            concat_slist = [d[:, :depth_size] for d in concat_slist]\n",
    "            #concat_var1_list = [d[:, :depth_size] for d in concat_var1_list]\n",
    "            concat_var2_list = [d[:, :depth_size] for d in concat_var2_list]\n",
    "            qc_list = [d[:, :depth_size] for d in qc_list]\n",
    "            min_depth_size = depth_size\n",
    "        elif depth_size > min_depth_size:\n",
    "            # 新しいデータを既存の深度に合わせて切り詰める\n",
    "            pdata = pdata[:, :min_depth_size]\n",
    "            sdata = sdata[:, :min_depth_size]\n",
    "            #var1_data = var1_data[:, :min_depth_size]\n",
    "            var2_data = var2_data[:, :min_depth_size]\n",
    "            qc_flags_str = qc_flags_str[:, :min_depth_size]\n",
    "\n",
    "        # データをリストに追加\n",
    "        concat_plist.append(pdata)\n",
    "        concat_slist.append(sdata)\n",
    "        #concat_var1_list.append(var1_data)\n",
    "        concat_var2_list.append(var2_data)\n",
    "        qc_list.append(qc_flags_str)\n",
    "        lon_list.append(lon_data)\n",
    "        lat_list.append(lat_data)\n",
    "        time_list.append(time)\n",
    "\n",
    "# リストを numpy 配列に結合\n",
    "concat_parray = np.concatenate(concat_plist, axis=0)\n",
    "concat_sarray = np.concatenate(concat_slist, axis=0)\n",
    "lon_array = np.array(lon_list)[:]\n",
    "lat_array = np.array(lat_list)[:]\n",
    "#concat_v1array = np.concatenate(concat_var1_list, axis=0)\n",
    "concat_v2qarray = np.concatenate(concat_var2_list, axis=0)\n",
    "concat_qcarray = np.concatenate(qc_list, axis=0)\n",
    "\n",
    "min_depth = int(concat_parray[:, -1].min())\n",
    "print(f\"Max interpolated depth is: {min_depth} dbar\")\n",
    "print(f\"Concatenated 'NITRATE_ADJUSTED_QC' data shape: {concat_v2qarray.shape}\")\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "depth_l = int(np.fix(concat_parray[:, -1].min()))\n",
    "\n",
    "depth_lerp = np.arange(0, depth_l, 1)\n",
    "\n",
    "# 補間後のデータを格納するための配列を作成\n",
    "#lerp_raw = np.empty((concat_v1array.shape[0], len(depth_lerp)))  # (時間ステップ, 深度範囲)\n",
    "lerp_adjusted_qc = np.empty((concat_v2qarray.shape[0], len(depth_lerp)))  # (時間ステップ, 深度範囲)\n",
    "\n",
    "# 各時間ステップで補間処理\n",
    "for i in range(concat_parray.shape[0]):\n",
    "    # NaNを除外した有効データを取得\n",
    "    valid_mask = ~np.isnan(concat_parray[i, :]) & ~np.isnan(concat_v2qarray[i, :])\n",
    "    valid_depth = concat_parray[i, valid_mask]\n",
    "    #valid_v1 = concat_v1array[i, valid_mask]\n",
    "    valid_v2 = concat_v2qarray[i, valid_mask]\n",
    "\n",
    "    # 線形補間を実行\n",
    "    v2_ldata = interpolate.interp1d(\n",
    "        valid_depth, \n",
    "        valid_v2, \n",
    "        bounds_error=False,  # 範囲外は補間せずにfill_valueを適用\n",
    "        fill_value=np.nan    # 範囲外のデータはNaNに設定\n",
    "    )\n",
    "    lerp_adjusted_qc[i, :] = v2_ldata(depth_lerp)\n",
    "\n",
    "print(lerp_adjusted_qc.shape)\n",
    "\n",
    "#print(lerp_raw[0, :50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Define the reference depth data from the minimum of temporal standalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有効な標準偏差を格納\n",
    "valid_std = []\n",
    "\n",
    "# 各深度ごとにチェック\n",
    "for i in range(500, lerp_adjusted_qc.shape[1]):\n",
    "    depth_data = lerp_adjusted_qc[:, i]  # 深度 i の時間系列データ\n",
    "    \n",
    "    if np.any(np.isnan(depth_data)):  \n",
    "        valid_std.append((i, np.inf))  # NaN を含む場合は無視（無限大の値をセット）\n",
    "    else:\n",
    "        valid_std.append((i, (np.nanstd(depth_data))/np.nanmean(depth_data)))  # NaN なしなら標準偏差を計算\n",
    "\n",
    "# 最小標準偏差の深度を特定\n",
    "best_depth = min(valid_std, key=lambda x: x[1])[0]\n",
    "\n",
    "print(f\"基準深度: {best_depth} dbar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Use the reference data from WOA2023 annual mean data\n",
    "\n",
    "##### download from https://www.ncei.noaa.gov/access/world-ocean-atlas-2023/bin/woa23oxnu.pl\n",
    "\n",
    "#### select NetCDF4 annual mean data ('woa23_all_n00_01a.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('woa23_all_n00_01a.nc','r')\n",
    "dataset.variables\n",
    "lon_woa=dataset.variables['lon'][:]\n",
    "lat_woa=dataset.variables['lat'][:]\n",
    "dep_woa=dataset.variables['depth'][:]\n",
    "n_an=dataset.variables['n_an'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Interploated the reference data. vertical: Akima horizontal: linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lat = np.arange(np.round(np.min(lat_array))-1, np.round(np.max(lat_array))+1, 1 / 10)  # 10分刻みの緯度\n",
    "new_lon = np.arange(np.round(np.min(lon_array))-1, np.round(np.max(lon_array))+1, 1 / 10)\n",
    "new_dep = np.arange(0, 5500, 1)\n",
    "\n",
    "w = int(np.round(np.min(lon_array))-1)\n",
    "e = int(np.round(np.max(lon_array))+1)\n",
    "s = int(np.round(np.min(lat_array))-1)\n",
    "n = int(np.round(np.max(lat_array))+1)\n",
    "\n",
    "n_data = n_an[0, :, s+90:n+90, w+180:e+180]\n",
    "\n",
    "interpolated_data = np.full((5500, n-s, e-w), np.nan)  # 全てを NaN で初期化\n",
    "\n",
    "for i in range(e-w): \n",
    "    for j in range(n-s):\n",
    "        depth_data = n_data[:, j, i]  # 位置(i, j)におけるデータ\n",
    "        akima_rst = interpolate.Akima1DInterpolator(dep_woa, depth_data)  # Akima補間\n",
    "        interpolated_data[:, j, i] = akima_rst(new_dep)  # 補間結果を格納\n",
    "ref_data_akima = interpolated_data\n",
    "ref_field_akima = interpolated_data[best_depth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ref_data_akima  # もともとのデータ (depth, lat, lon)\n",
    "lon_points = lon_woa[w + 180:e + 180]\n",
    "lat_points = lat_woa[s + 90:n + 90]\n",
    "\n",
    "# 空の配列を作成\n",
    "interpolated_data = np.empty((5500, len(new_lat), len(new_lon)))\n",
    "\n",
    "# lon方向の線形補間\n",
    "lon_interpolated_data = np.empty((5500, len(lat_points), len(new_lon)))\n",
    "\n",
    "for k in range(0, 5500):\n",
    "    for j in range(len(lat_points)):\n",
    "        depth_data = data[k, j, :]  # 深さ k におけるデータ (lat, lon)\n",
    "        # NaN を除外する\n",
    "        valid_mask = ~np.isnan(depth_data)\n",
    "        if valid_mask.sum() > 1:  # 有効データが2点以上ある場合のみ補間\n",
    "            linear_lon = interpolate.interp1d(\n",
    "                lon_points[valid_mask], depth_data[valid_mask],\n",
    "                bounds_error=False, fill_value=\"extrapolate\"\n",
    "            )\n",
    "            lon_interpolated_data[k, j, :] = linear_lon(new_lon)  # lon方向に補間\n",
    "        else:\n",
    "            lon_interpolated_data[k, j, :] = np.nan  # NaN のまま保持\n",
    "\n",
    "# lat方向の線形補間\n",
    "for k in range(0, 5500):\n",
    "    for i in range(len(new_lon)):\n",
    "        depth_data = lon_interpolated_data[k, :, i]  # lon方向補間後の lat データ\n",
    "        # NaN を除外する\n",
    "        valid_mask = ~np.isnan(depth_data)\n",
    "        if valid_mask.sum() > 1:\n",
    "            linear_lat = interpolate.interp1d(\n",
    "                lat_points[valid_mask], depth_data[valid_mask],\n",
    "                bounds_error=False, fill_value=\"extrapolate\"\n",
    "            )\n",
    "            interpolated_data[k, :, i] = linear_lat(new_lat)  # lat方向に補間\n",
    "        else:\n",
    "            interpolated_data[k, :, i] = np.nan  # NaN のまま保持\n",
    "\n",
    "# lat, lon方向に補間が完了したデータ\n",
    "print(interpolated_data.shape)\n",
    "\n",
    "mask = interpolated_data >= 500 #陸地\n",
    "interpolated_data[mask] = np.nan\n",
    "\n",
    "ref_dep_lerpdata = interpolated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### This is the shape of reference data based on WOA2023 annual mean data at ref depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_var = ref_dep_lerpdata[best_depth]\n",
    "\n",
    "vmin = 38\n",
    "vmax = 41\n",
    "vint = 0.2\n",
    "\n",
    "fig = plt.figure()\n",
    "rcParams['figure.figsize'] = 8,6\n",
    "c_lon = 180\n",
    "ax = fig.add_subplot(1,1,1, projection=ccrs.PlateCarree(central_longitude=c_lon))\n",
    "\n",
    "\n",
    "cm = plt.get_cmap('Purples') \n",
    "cs = ax.contourf(new_lon, new_lat, draw_var, cmap=cm, norm=Normalize(vmin=vmin, vmax=vmax),\\\n",
    "                  levels=np.arange(vmin,vmax+vint,vint), extend='both', \\\n",
    "                  transform=ccrs.PlateCarree())\n",
    "linecs = ax.contour(new_lon-180, new_lat, draw_var, colors='k',levels=np.arange(0,5,1), linewidths=1) #等値線\n",
    "ax.clabel(linecs, fontsize=15, inline=True, inline_spacing=0.2, use_clabeltext=True)\n",
    "\n",
    "ax.plot(lon_array[:, 0],lat_array[:, 0], color = 'b', lw = '1.5', transform=ccrs.PlateCarree())\n",
    "\n",
    "ax.plot(lon_array[0],lat_array[0], 'ob', markersize = '8', label = 'start', transform=ccrs.PlateCarree())\n",
    "ax.plot(lon_array[-1],lat_array[-1], 'xb', markersize = '8', label = 'end', transform=ccrs.PlateCarree())\n",
    "\n",
    "\n",
    "dlon,dlat=5,5\n",
    "xticks=np.arange(60,360.1,dlon)\n",
    "yticks=np.arange(-60,70.1,dlat)\n",
    "ax.set_title('All WOA ref depth (1341 dbar)', fontsize=20)\n",
    "ax.set_xticks(xticks,crs=ccrs.PlateCarree())\n",
    "ax.set_yticks(yticks,crs=ccrs.PlateCarree())\n",
    "ax.coastlines(lw=1.0,color='black',resolution='50m')\n",
    "latfmt=LatitudeFormatter()\n",
    "lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
    "ax.xaxis.set_major_formatter(lonfmt)\n",
    "ax.yaxis.set_major_formatter(latfmt)\n",
    "ax.axes.tick_params(labelsize=20)\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree(), linewidth=0.5, linestyle=':', color='k', alpha=0.8)\n",
    "gl.xlocator = mticker.FixedLocator(np.arange(-180, 180, 5))\n",
    "gl.ylocator = mticker.FixedLocator(np.arange(-90, 90, 5))\n",
    "cbar = plt.colorbar(cs, shrink=0.6) \n",
    "cbar.ax.tick_params(labelsize=20)\n",
    "cbar.set_label(r\"$\\mu$mol/kg\", size=20) \n",
    "\n",
    "ax.set_extent([w-5, e+5, s-5, n+5], crs=ccrs.PlateCarree())\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### フロートの位置に最も近い new_lon, new_lat のインデックスを探す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_lon_idx = np.array([np.abs(new_lon - lon).argmin() for lon in lon_array.flatten()])\n",
    "nearest_lat_idx = np.array([np.abs(new_lat - lat).argmin() for lat in lat_array.flatten()])\n",
    "\n",
    "# interpolated_data から最も近い格子点の時系列データを取得\n",
    "# (深さ 5500 のデータを時系列で取得)\n",
    "woa_time_series = np.array([\n",
    "    ref_dep_lerpdata[:, nearest_lat_idx[i], nearest_lon_idx[i]] for i in range(len(lon_array))\n",
    "])\n",
    "\n",
    "# 結果の形状確認\n",
    "print(woa_time_series.shape)  # (時系列の長さ, 5500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Drawing the timeseries of the profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_var2 = lerp_adjusted_qc.transpose(1, 0)\n",
    "draw_var1 = woa_time_series.transpose(1, 0)\n",
    "draw_var3 = (lerp_adjusted_qc-woa_time_series[:, :min_depth]).transpose(1, 0)\n",
    "    \n",
    "vmin = 0\n",
    "vmax = 50\n",
    "vint = 5\n",
    "    \n",
    "cm = plt.get_cmap('Purples') \n",
    "    \n",
    "rcParams['figure.figsize'] = 8,7\n",
    "fig = plt.figure()\n",
    "fig.suptitle(f\"{fnum}\", fontsize=20, x=0.45)\n",
    "\n",
    "ax2 = fig.add_subplot(3,1,1)\n",
    "cs = plt.contourf(time_list, depth_lerp, draw_var2, \n",
    "                  cmap=cm, norm=Normalize(vmin=vmin, vmax=vmax),\n",
    "                  levels=np.arange(vmin,vmax+vint,vint), extend='both')\n",
    "\n",
    "ax2.set_title(f\"{var2}\", fontsize=18)\n",
    "ax2.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax2.xaxis.set_major_locator(mdates.MonthLocator(interval = 1))\n",
    "ax2.tick_params(axis='x', rotation=45)  # 日付ラベルを45度傾けて見やすくする\n",
    "ax2.set_yticks(np.linspace(0, 1500, num=6))\n",
    "ax2.axes.tick_params(labelsize=12)\n",
    "#ax2.set_yticklabels([\"200\",\"150\",\"100\",\"50\",\"0\"])\n",
    "ax2.set_ylabel(\"Depth (dbar)\", size=12)\n",
    "ax2.grid(linewidth=1, linestyle=':', color='k', alpha=0.8)\n",
    "ax2.invert_yaxis()  # 深度を下向きにする\n",
    "ax2.minorticks_on()\n",
    "ax2.set_ylim(1500, 0)\n",
    "    \n",
    "cbar = plt.colorbar(cs, shrink=0.9) \n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "cbar.set_label(r\"$\\mu$mol/kg\", size=12)\n",
    "\n",
    "ax1 = fig.add_subplot(3,1,2)\n",
    "cs = plt.contourf(time_list, new_dep, draw_var1, \n",
    "                  cmap=cm, norm=Normalize(vmin=vmin, vmax=vmax),\n",
    "                  levels=np.arange(vmin,vmax+vint,vint), extend='both')\n",
    "\n",
    "ax1.set_title(\"WOA2023\", fontsize=18)\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax1.xaxis.set_major_locator(mdates.MonthLocator(interval = 1))\n",
    "ax1.tick_params(axis='x', rotation=45)  # 日付ラベルを45度傾けて見やすくする\n",
    "ax1.set_yticks(np.linspace(0, 1500, num=6))\n",
    "ax1.axes.tick_params(labelsize=12)\n",
    "#ax1.set_yticklabels([\"200\",\"150\",\"100\",\"50\",\"0\"])\n",
    "ax1.set_ylabel(\"Depth (dbar)\", size=12)\n",
    "ax1.grid(linewidth=1, linestyle=':', color='k', alpha=0.8)\n",
    "ax1.invert_yaxis()  # 深度を下向きにする\n",
    "ax1.minorticks_on()\n",
    "ax1.set_ylim(1500, 0)\n",
    "    \n",
    "cbar = plt.colorbar(cs, shrink=0.9) \n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "cbar.set_label(r\"$\\mu$mol/kg\", size=12)\n",
    "\n",
    "ax3 = fig.add_subplot(3,1,3)\n",
    "\n",
    "vmin = -4\n",
    "vmax = 4\n",
    "vint = 0.5\n",
    "\n",
    "cs = plt.contourf(time_list, depth_lerp, draw_var3, \n",
    "                  cmap=plt.get_cmap('seismic'), norm=Normalize(vmin=vmin, vmax=vmax),\n",
    "                  levels=np.arange(vmin,vmax+vint,vint), extend='both')\n",
    "\n",
    "ax3.set_title(\"Difference\", fontsize=18)\n",
    "ax3.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax3.xaxis.set_major_locator(mdates.MonthLocator(interval = 1))\n",
    "ax3.tick_params(axis='x', rotation=45)  # 日付ラベルを45度傾けて見やすくする\n",
    "ax3.set_yticks(np.linspace(0, 1500, num=6))\n",
    "ax3.axes.tick_params(labelsize=12)\n",
    "#ax3.set_yticklabels([\"200\",\"150\",\"100\",\"50\",\"0\"])\n",
    "ax3.set_ylabel(\"Depth (dbar)\", size=12)\n",
    "ax3.grid(linewidth=1, linestyle=':', color='k', alpha=0.8)\n",
    "ax3.minorticks_on()\n",
    "ax3.invert_yaxis()  # 深度を下向きにする\n",
    "ax3.set_ylim(1500, 0)\n",
    "    \n",
    "cbar = plt.colorbar(cs, shrink=0.9) \n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "cbar.set_label(r\"$\\mu$mol/kg\", size=12)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Calculate the difference between the float data and WOA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 8,5\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "\n",
    "#ax1.scatter(x, diff_Data, color=\"green\", edgecolor=\"black\", zorder=2)\n",
    "ax.scatter(time_list, lerp_adjusted_qc[:, best_depth], s= 25, label = f\"{fnum}\")\n",
    "ax.scatter(time_list, woa_time_series[:, best_depth], s= 25, label = 'WOA23 CLIM')\n",
    "ax.set_title(f\"Nitrate at {best_depth} dbar\", fontsize=18)\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(interval = 1))\n",
    "ax.tick_params(axis='x', rotation=45)  # 日付ラベルを45度傾けて見やすくする\n",
    "#yticks = np.arange(-0.02, 0.021, 0.01)\n",
    "#ax.set_yticks(yticks)\n",
    "ax.axes.tick_params(labelsize=12)\n",
    "#ax3.set_yticklabels([\"200\",\"150\",\"100\",\"50\",\"0\"])]\n",
    "ax.legend(fontsize=10, ncol=3)\n",
    "ax.set_ylabel((r\"$\\mu$mol/kg\"), size=12)\n",
    "ax.grid(axis = 'x', linewidth=1, linestyle=':', color='k', alpha=0.8)\n",
    "ax.minorticks_on()\n",
    "ax.set_ylim(37, 42)\n",
    "\n",
    "ax = fig.add_subplot(2,1,2)\n",
    "\n",
    "ax.scatter(time_list, lerp_adjusted_qc[:, best_depth]-woa_time_series[:, best_depth], s= 25, c = 'k', label = \"Difference\")\n",
    "#ax.plot(time_list, woa_time_series, label = 'WOA23 CLIM')\n",
    "#ax.set_title(f\"Diff at {fnum} {best_depth} dbar\", fontsize=18)\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(interval = 1))\n",
    "ax.tick_params(axis='x', rotation=45)  # 日付ラベルを45度傾けて見やすくする\n",
    "#yticks = np.arange(-0.02, 0.021, 0.01)\n",
    "#ax.set_yticks(yticks)\n",
    "ax.axes.tick_params(labelsize=12)\n",
    "#ax3.set_yticklabels([\"200\",\"150\",\"100\",\"50\",\"0\"])]\n",
    "ax.legend(loc= (0.05, 0.05), fontsize=10, ncol=3)\n",
    "ax.set_ylabel((r\"$\\mu$mol/kg\"), size=12)\n",
    "ax.grid(axis = 'x', linewidth=1, linestyle=':', color='k', alpha=0.8)\n",
    "ax.minorticks_on()\n",
    "ax.set_ylim(-4, 4)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Define the break points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_Data = lerp_adjusted_qc[:, best_depth]-woa_time_series[:, best_depth]\n",
    "\n",
    "# **データの準備**\n",
    "n_cycles = diff_Data.shape[0]\n",
    "x = np.arange(n_cycles)  # cycle のインデックス\n",
    "\n",
    "# **Binary segmentation 実行**\n",
    "algo = rpt.Binseg(model=\"l2\", min_size=max(1, n_cycles//10)).fit(diff_Data)\n",
    "possible_bkps = algo.predict(n_bkps=(n_cycles//10) -1)  # セグメント数 = (n数/10)の整数部分 -1 \n",
    "\n",
    "# **ブレークポイントの前処理**\n",
    "if isinstance(possible_bkps, int):  \n",
    "    possible_bkps = [0, possible_bkps, n_cycles]\n",
    "elif not isinstance(possible_bkps, list):  \n",
    "    raise ValueError(\"Breakpoints should be a list, but got:\", possible_bkps)\n",
    "\n",
    "if possible_bkps[0] != 0:\n",
    "    possible_bkps.insert(0, 0)\n",
    "if possible_bkps[-1] != n_cycles:\n",
    "    possible_bkps.append(n_cycles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Defeine the Drifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **X軸をdatetimeのまま保持し、回帰には数値を使う**\n",
    "x = np.array(time_list)  \n",
    "x_numeric = mdates.date2num(x)  # datetime を float に変換\n",
    "\n",
    "# **グラフの設定**\n",
    "rcParams['figure.figsize'] = 7,3\n",
    "fig = plt.figure()\n",
    "fig.suptitle(f\"{fnum}\", fontsize=18, x=0.55, y=0.92)\n",
    "\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "ax1.set_title(\"Difference\", fontsize=18)\n",
    "ax1.scatter(x, diff_Data, color=\"g\")\n",
    "ax1.set_ylabel(r\"$\\mu$mol/kg\", size=12)\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax1.xaxis.set_major_locator(mdates.MonthLocator(interval=1))  # 1ヶ月ごとにラベルを表示\n",
    "ax1.axes.tick_params(labelsize=12)\n",
    "ax1.tick_params(axis='x', rotation=45)  # 日付ラベルを45度傾けて見やすくする\n",
    "ax1.grid(axis = 'x', linestyle=\"--\", alpha=0.8)\n",
    "\n",
    "# **各セグメントで最小二乗法を適用**\n",
    "drift = []\n",
    "prev_bkp = 0\n",
    "\n",
    "for bkp in possible_bkps[1:]:\n",
    "    x_segment = x_numeric[prev_bkp:bkp]  # **回帰用に float を使う**\n",
    "    y_segment = diff_Data[prev_bkp:bkp]\n",
    "\n",
    "    # **線形回帰（datetime ではなく float を使う）**\n",
    "    model = LinearRegression().fit(x_segment.reshape(-1, 1), y_segment)\n",
    "    y_pred = model.predict(x_segment.reshape(-1, 1))\n",
    "\n",
    "    # **プロット用に元の datetime に戻す**\n",
    "    ax1.plot(x[prev_bkp:bkp], y_pred, color=\"gray\", linewidth=2, zorder=1)\n",
    "\n",
    "    # **セグメント開始点に破線を追加**\n",
    "    ax1.axvline(x[prev_bkp], color=\"purple\", linestyle=\"dashed\", linewidth=2)\n",
    "    prev_bkp = bkp\n",
    "    drift.append(y_pred)\n",
    "\n",
    "model_diff = np.concatenate(drift)\n",
    "modify_adjustedData = lerp_adjusted_qc-model_diff[:, np.newaxis]\n",
    "\n",
    "# **ブレークポイントの出力**\n",
    "for i in range(len(possible_bkps)-1):  \n",
    "    print(\"Detected breakpoints:\", time_list[possible_bkps[i]])\n",
    "    \n",
    "print(\"Adjusted difference mean = \", np.mean(np.abs(diff_Data)))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### subtracte the difference from the float data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_modiData = modify_adjustedData[:, best_depth]-woa_time_series[:, best_depth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **X軸をdatetimeのまま保持し、回帰には数値を使う**\n",
    "x = np.array(time_list)  \n",
    "x_numeric = mdates.date2num(x)  # datetime を float に変換\n",
    "\n",
    "# **グラフの設定**\n",
    "rcParams['figure.figsize'] = 7,3\n",
    "fig = plt.figure()\n",
    "fig.suptitle(f\"{fnum}\", fontsize=18, x=0.55, y=0.92)\n",
    "\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "ax1.set_title(\"Difference\", fontsize=18)\n",
    "ax1.scatter(x, diff_modiData, color=\"r\")\n",
    "ax1.set_ylabel(r\"$\\mu$mol/kg\", size=12)\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax1.xaxis.set_major_locator(mdates.MonthLocator(interval=1))  # 1ヶ月ごとにラベルを表示\n",
    "ax1.axes.tick_params(labelsize=12)\n",
    "ax1.tick_params(axis='x', rotation=45)  # 日付ラベルを45度傾けて見やすくする\n",
    "yticks=np.arange(-0.5, 0.51, 0.25)\n",
    "ax1.set_yticks(yticks)\n",
    "ax1.grid(axis = 'x', linestyle=\"--\", alpha=0.8)\n",
    "ax1.set_ylim(-0.6, 0.6)\n",
    "ax1.hlines(y=0.5, xmin=0, xmax=1, lw = 0.5, color='k', alpha=0.8, transform=ax1.transAxes)\n",
    "\n",
    "print(\"Corrected difference = \", np.mean(np.abs(diff_modiData)))\n",
    "\n",
    "# **各セグメントで最小二乗法を適用**\n",
    "drift = []\n",
    "prev_bkp = 0\n",
    "\n",
    "for bkp in possible_bkps[1:]:\n",
    "    x_segment = x_numeric[prev_bkp:bkp]  # **回帰用に float を使う**\n",
    "    y_segment = diff_Data[prev_bkp:bkp]\n",
    "\n",
    "    # **線形回帰（datetime ではなく float を使う）**\n",
    "    model = LinearRegression().fit(x_segment.reshape(-1, 1), y_segment)\n",
    "    y_pred = model.predict(x_segment.reshape(-1, 1))\n",
    "\n",
    "    # **セグメント開始点に破線を追加**\n",
    "    ax1.axvline(x[prev_bkp], color=\"purple\", linestyle=\"dashed\", linewidth=2)\n",
    "    prev_bkp = bkp\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Drawing the timeseries of the corrected profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_var = modify_adjustedData.transpose(1, 0)\n",
    "    \n",
    "vmin = 0\n",
    "vmax = 50\n",
    "vint = 5\n",
    "    \n",
    "cm = plt.get_cmap('Purples') \n",
    "    \n",
    "rcParams['figure.figsize'] = 8,3\n",
    "fig = plt.figure()\n",
    "fig.suptitle(f\"{fnum}\", fontsize=18, x=0.45, y=0.92)\n",
    "\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "cs = plt.contourf(time_list, depth_lerp, draw_var, \n",
    "                  cmap=cm, norm=Normalize(vmin=vmin, vmax=vmax),\n",
    "                  levels=np.arange(vmin,vmax+vint,vint), extend='both')\n",
    "\n",
    "ax.set_title(\"Corrected data\", fontsize=18)\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(interval = 1))\n",
    "ax.tick_params(axis='x', rotation=45)  # 日付ラベルを45度傾けて見やすくする\n",
    "ax.set_yticks(np.linspace(0, 1500, num=6))\n",
    "ax.axes.tick_params(labelsize=12)\n",
    "ax.set_ylabel(\"Depth (dbar)\", size=12)\n",
    "ax.grid(linewidth=1, linestyle=':', color='k', alpha=0.8)\n",
    "ax.invert_yaxis()  # 深度を下向きにする\n",
    "ax.minorticks_on()\n",
    "ax.set_ylim(1500, 0)\n",
    "    \n",
    "cbar = plt.colorbar(cs, shrink=0.9) \n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "cbar.set_label(r\"$\\mu$mol/kg\", size=12)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "modify_adjustedData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
