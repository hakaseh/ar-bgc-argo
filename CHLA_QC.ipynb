{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Generating the analysis-ready BGC-Argo dataset\n",
    "### Haruto Fujishima, Hakase Hayashida (Application Laboratory, JAMSTEC)\n",
    "\n",
    "##### Biogeochemical Argo (BGC-Argo) is a global array of autonomous profiling floats that monitor marine ecosystem health in the upper 2,000 meters of the water column on a weekly basis. Here we develop such a post-processing tool that generates “analysis-ready” time series of biogeochemical profiles from a selected float. This tool is written in Python and uses Jupyter Notebook. Users can produce, visualize, and analyze the time series of their interest based on the time, geographic region, and biogeochemical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc4\n",
    "import statistics\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from scipy.ndimage import generic_filter\n",
    "import matplotlib.dates as mdates\n",
    "import multiprocessing\n",
    "from scipy import interpolate\n",
    "from pylab import rcParams\n",
    "from matplotlib.colors import Normalize\n",
    "from netCDF4 import Dataset\n",
    "import glob\n",
    "import gsw\n",
    "import pvlib\n",
    "from pvlib.location import Location\n",
    "plt.rcParams['font.family'] = 'Times New Roman'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Preparation for data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://usgodae.org/ftp/outgoing/argo/argo_synthetic-profile_index.txt',skiprows=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Define spatial and temporal coverages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon0 = 130\n",
    "lon1 = 150\n",
    "lat0 = 20\n",
    "lat1 = 50\n",
    "date0 = 20240101\n",
    "date1 = 20241231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the dataset based on lon, lat, and variable (chl). Date has a weird format so extract first then postprocess dates\n",
    "datasub = data[(data['longitude'] >= lon0) & (data['longitude'] <= lon1) & (data['latitude'] >= lat0) &\n",
    "               (data['latitude'] <= lat1) & (data['date'] >= 0) & (data['parameters'].str.contains(' CHLA '))]\n",
    "\n",
    "#add the new variable called 'hour' referring to the hour of sampling\n",
    "#extract the two digits after yyyymmdd corresponding to hh of hhmmss\n",
    "hourstr = [str(num) for num in datasub['date']]\n",
    "hourstr = [num[8:8+2] for num in hourstr]\n",
    "hourint = [int(num) for num in hourstr]\n",
    "datasub['hour'] = hourint\n",
    "\n",
    "#add the new variable called 'time' referring to the hour of sampling\n",
    "#extract the 6 digits after yyyymmdd corresponding to hhmmss\n",
    "#NOTE: time is provided as string to avoid inconsistent digits for hours\n",
    "timestr = [str(num) for num in datasub['date']]\n",
    "timestr = [num[8:8+6] for num in timestr]\n",
    "datasub['time'] = timestr\n",
    "\n",
    "#modify the date format to be yyyymmdd\n",
    "datestr = [str(num) for num in datasub['date']]\n",
    "datestr = [num[0:8] for num in datestr]\n",
    "dateint = [int(num) for num in datestr]\n",
    "datasub['date'] = dateint\n",
    "#create a new column called floatid for lagrangian time series plot\n",
    "tmpfloatid = []\n",
    "for i in range(np.size(datasub['date'])):\n",
    "    tmpfloatid.append(datasub['file'].values[i].split('/')[1])\n",
    "datasub['floatid'] = tmpfloatid\n",
    "#refine the dataset based on the selected period\n",
    "datasub = datasub[(datasub['date'].values >= date0) & (datasub['date'].values <= date1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Check the number of floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasub['file']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(datasub.shape[0]):\n",
    "    dir = 'ftp.ifremer.fr/ifremer/argo/dac/'+datasub['file'].values[i]\n",
    "    !curl -L -O {dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## The following is a correction of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Choose the float you use\n",
    "##### For example, SR2902878, SD5906510..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnum = 'SD5906519'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "##### Function that calculates the solar elevation angle, used for NPQ correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_solar_elevation(lat, lon, utc):\n",
    "    \"\"\"緯度, 経度, UTC時間 から太陽光角度を計算\"\"\"\n",
    "    location = pvlib.location.Location(lat, lon)\n",
    "    solar_position = location.get_solarposition(utc)\n",
    "    solar_zenith = solar_position['zenith'].values[0]  # 太陽天頂角\n",
    "    solar_elevation = 90 - solar_zenith  # 太陽高度角\n",
    "    return solar_elevation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Read datas from all profiles from the float you choosed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = f\"{fnum}_*.nc\"\n",
    "\n",
    "# ファイルパターンに一致するすべてのファイルを取得\n",
    "file_list = sorted(glob.glob(file_pattern))  # ソートして番号順に処理\n",
    "if not file_list:\n",
    "    raise FileNotFoundError(f\"No files found matching pattern: {file_pattern}\")\n",
    "    \n",
    "# データを格納するリスト\n",
    "concat_plist = []\n",
    "concat_slist = []\n",
    "concat_tlist = []\n",
    "solar_angle_list = []\n",
    "concat_var2_list = []\n",
    "qc_list = []\n",
    "lon_list = []\n",
    "lat_list = []\n",
    "time_list = []\n",
    "min_depth_size = None\n",
    "base_date = datetime(1950, 1, 1)\n",
    "\n",
    "# ファイルを順番に読み込む\n",
    "for file_name in file_list:        \n",
    "    # netCDF ファイルを開く\n",
    "    with Dataset(file_name, mode='r') as nc:\n",
    "        if \"CHLA_ADJUSTED\" not in nc.variables:\n",
    "            print(f\"Variable 'CHLA_ADJUSTED' not found in {file_name}\")\n",
    "            continue\n",
    "\n",
    "        # QC フラグの取得とチェック\n",
    "        if \"CHLA_ADJUSTED_QC\" in nc.variables:\n",
    "            qc_flags = nc.variables[\"CHLA_ADJUSTED_QC\"][:]\n",
    "            qc_flags_str = np.where(qc_flags.mask, ' ', qc_flags.astype(str))  # マスク部分は空白\n",
    "            valid_qc_mask = np.isin(qc_flags_str, ['1', '2', '5', '8'])  # QCが1,2,5のもの\n",
    "        else:\n",
    "            print(f\"CHLA_ADJUSTED_QC が {file_name} に存在しません。\")\n",
    "            continue\n",
    "\n",
    "        # QC フラグの取得とチェック\n",
    "        if \"TEMP_QC\" in nc.variables:\n",
    "            tqc_flags = nc.variables[\"TEMP_QC\"][:]\n",
    "            sqc_flags = nc.variables[\"PSAL_QC\"][:]\n",
    "            tqc_flags_str = np.where(tqc_flags.mask, ' ', tqc_flags.astype(str))  # マスク部分は空白\n",
    "            valid_tqc_mask = np.isin(tqc_flags_str, ['1', '2'])  # QCが1,2のもの\n",
    "            sqc_flags_str = np.where(sqc_flags.mask, ' ', sqc_flags.astype(str))  # マスク部分は空白\n",
    "            valid_sqc_mask = np.isin(sqc_flags_str, ['1', '2'])  # QCが1,2のもの\n",
    "        else:\n",
    "            print(f\"TEMP_QC が {file_name} に存在しません。\")\n",
    "            continue\n",
    "        \n",
    "        # 指定された変数のデータを取得\n",
    "        # 時間ステップ\n",
    "        juld = nc.variables[\"JULD\"][:][0]\n",
    "        time = base_date + timedelta(days=juld)\n",
    "\n",
    "        # 緯度・経度データ\n",
    "        lat = nc.variables[\"LATITUDE\"][:][0]\n",
    "        lon = nc.variables[\"LONGITUDE\"][:][0]\n",
    "\n",
    "        # 変数\n",
    "        pdata = nc.variables[\"PRES\"][:, :]\n",
    "        sdata = nc.variables[\"PSAL\"][:, :]\n",
    "        tdata = nc.variables[\"TEMP\"][:, :]\n",
    "\n",
    "        var2_data = nc.variables[\"CHLA_ADJUSTED\"][:, :]\n",
    "        lon_data = nc.variables[\"LONGITUDE\"][:]\n",
    "        lat_data = nc.variables[\"LATITUDE\"][:]\n",
    "        # mask\n",
    "        pdata = np.where(pdata >= 99999.0, np.nan, pdata)\n",
    "        var2_data = np.where(var2_data >= 99999.0, np.nan, var2_data)\n",
    "\n",
    "        # QC フラグでフィルタリング\n",
    "        var2_data[~valid_qc_mask] = np.nan\n",
    "        tdata[~valid_tqc_mask] = np.nan\n",
    "        sdata[~valid_sqc_mask] = np.nan\n",
    "\n",
    "\n",
    "        # 深度方向のサイズを取得\n",
    "        depth_size = pdata.shape[1]        \n",
    "        # 最初のファイルの場合、最小深度を初期化\n",
    "        if min_depth_size is None:\n",
    "            min_depth_size = depth_size\n",
    "        \n",
    "        # 深度方向のサイズを調整\n",
    "        if depth_size < min_depth_size:\n",
    "            # すでに存在するデータを新しい最小深度に切り詰める\n",
    "            concat_plist = [d[:, :depth_size] for d in concat_plist]\n",
    "            concat_slist = [d[:, :depth_size] for d in concat_slist]\n",
    "            concat_tlist = [d[:, :depth_size] for d in concat_tlist]\n",
    "            concat_var2_list = [d[:, :depth_size] for d in concat_var2_list]\n",
    "            qc_list = [d[:, :depth_size] for d in qc_list]\n",
    "            min_depth_size = depth_size\n",
    "        elif depth_size > min_depth_size:\n",
    "            # 新しいデータを既存の深度に合わせて切り詰める\n",
    "            pdata = pdata[:, :min_depth_size]\n",
    "            sdata = sdata[:, :min_depth_size]\n",
    "            tdata = tdata[:, :min_depth_size]\n",
    "            var2_data = var2_data[:, :min_depth_size]\n",
    "            qc_flags_str = qc_flags_str[:, :min_depth_size]\n",
    "\n",
    "        # 太陽光角度を計算\n",
    "        solar_angle = calc_solar_elevation(lat, lon, time)\n",
    "\n",
    "        # データをリストに追加\n",
    "        concat_plist.append(pdata)\n",
    "        concat_slist.append(sdata)\n",
    "        concat_tlist.append(tdata)\n",
    "        concat_var2_list.append(var2_data)\n",
    "        solar_angle_list.append(solar_angle)\n",
    "        qc_list.append(qc_flags_str)\n",
    "        lon_list.append(lon_data)\n",
    "        lat_list.append(lat_data)\n",
    "        time_list.append(time)\n",
    "\n",
    "# リストを numpy 配列に結合\n",
    "concat_parray = np.concatenate(concat_plist, axis=0)\n",
    "concat_sarray = np.concatenate(concat_slist, axis=0)\n",
    "concat_tarray = np.concatenate(concat_tlist, axis=0)\n",
    "angle_array = np.array(solar_angle_list)\n",
    "lon_array = np.array(lon_list)[:]\n",
    "lat_array = np.array(lat_list)[:]\n",
    "concat_v2qarray = np.concatenate(concat_var2_list, axis=0)\n",
    "concat_qcarray = np.concatenate(qc_list, axis=0)\n",
    "\n",
    "print(f\"Max interpolated depth is: {concat_parray[:, -1].min()}\")\n",
    "print(f\"Concatenated 'CHLA_ADJUSTED_QC' data shape: {concat_v2qarray.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Function that calculates the median filter, used for smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN を無視する中央値フィルタ関数\n",
    "def nanmedian_filter(values):\n",
    "    valid_values = values[~np.isnan(values)]  # NaN を除去\n",
    "    return np.median(valid_values) if len(valid_values) > 0 else np.nan  # 有効値があれば中央値、なければ NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#### Smoothing based on vertical resolution and interpolating data vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各プロファイルごとに異なる窓サイズでフィルタを適用\n",
    "smoothed_v2qarray_fixed = np.empty_like(concat_v2qarray)\n",
    "\n",
    "for i in range(concat_v2qarray.shape[0]):  # 各プロファイルごとに処理\n",
    "    pres_res = np.median(np.diff(concat_parray[i, :]))  # 深度の解像度を計算\n",
    "\n",
    "    # 窓サイズの決定\n",
    "    if pres_res >= 3:\n",
    "        nsmooth = 5\n",
    "    elif pres_res <= 1:\n",
    "        nsmooth = 11\n",
    "    else:\n",
    "        nsmooth = 7\n",
    "\n",
    "    # 中央値フィルタを適用\n",
    "    smoothed_v2qarray_fixed[i, :] = generic_filter(\n",
    "        concat_v2qarray[i, :], nanmedian_filter, size=nsmooth, mode='nearest')\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "depth_l = int(np.fix(concat_parray[:, -1].min()))\n",
    "\n",
    "depth_lerp = np.arange(0, depth_l, 1)\n",
    "\n",
    "# 補間後のデータを格納するための配列を作成\n",
    "lerp_v2data_adjusted_smqc = np.empty((smoothed_v2qarray_fixed.shape[0], len(depth_lerp)))  # (時間ステップ, 深度範囲)\n",
    "\n",
    "# 各時間ステップで補間処理\n",
    "for i in range(concat_parray.shape[0]):\n",
    "    # NaNを除外した有効データを取得\n",
    "    valid_mask = ~np.isnan(smoothed_v2qarray_fixed[i, :])\n",
    "    valid_depth = concat_parray[i, valid_mask]\n",
    "    valid_v2 = smoothed_v2qarray_fixed[i, valid_mask]\n",
    "\n",
    "    # 有効データがない場合、補間をスキップ\n",
    "    if valid_v2.size == 0 or np.all(valid_v2 == 0):\n",
    "        lerp_v2data_adjusted_smqc[i, :] = np.nan  # 補間結果をNaNに設定\n",
    "        continue\n",
    "\n",
    "    # 線形補間を実行\n",
    "    v2_ldata = interpolate.interp1d(\n",
    "        valid_depth, \n",
    "        valid_v2, \n",
    "        bounds_error=False,  # 範囲外は補間せずにfill_valueを適用\n",
    "        fill_value=np.nan    # 範囲外のデータはNaNに設定\n",
    "    )\n",
    "    lerp_v2data_adjusted_smqc[i, :] = v2_ldata(depth_lerp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### Calculation of potential temperature, potential salinity, and potential density to define MLD and interpolating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 絶対塩分（Absolute Salinity, SA）の計算\n",
    "SA = gsw.SA_from_SP(concat_sarray, concat_parray, lon_array, lat_array)\n",
    "\n",
    "# 2. 実効温度（Conservative Temperature, CT）の計算\n",
    "CT = gsw.CT_from_t(SA, concat_tarray, concat_parray)\n",
    "\n",
    "# 3. ポテンシャル密度（Potential Density, σθ）の計算（基準圧力 0 dbar）\n",
    "concat_stheta_array = gsw.sigma0(SA, CT)  # σθ = 密度 - 1000 (kg/m³)\n",
    "\n",
    "depth_l = int(np.fix(concat_parray[:, -1].min()))\n",
    "\n",
    "depth_lerp = np.arange(0, depth_l, 1)\n",
    "\n",
    "# 補間後のデータを格納するための配列を作成\n",
    "lerp_ddata = np.empty((concat_stheta_array.shape[0], len(depth_lerp)))  # (時間ステップ, 深度範囲)\n",
    "\n",
    "# 各時間ステップで補間処理\n",
    "for i in range(concat_stheta_array.shape[0]):\n",
    "    # NaNを除外した有効データを取得\n",
    "    valid_mask = ~np.isnan(concat_stheta_array[i, :])\n",
    "    valid_depth = concat_parray[i, valid_mask]\n",
    "    valid_v2 = concat_stheta_array[i, valid_mask]\n",
    "\n",
    "    # 有効データがない場合、補間をスキップ\n",
    "    if valid_v2.size == 0 or np.all(valid_v2 == 0):\n",
    "        lerp_ddata[i, :] = np.nan  # 補間結果をNaNに設定\n",
    "        continue\n",
    "\n",
    "    # 線形補間を実行\n",
    "    v2_ldata = interpolate.interp1d(\n",
    "        valid_depth, \n",
    "        valid_v2, \n",
    "        bounds_error=False,  # 範囲外は補間せずにfill_valueを適用\n",
    "        fill_value=np.nan    # 範囲外のデータはNaNに設定\n",
    "    )\n",
    "    lerp_ddata[i, :] = v2_ldata(depth_lerp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "#### Define the MLD based on the difference of potential density ~0.03 kg/m3\n",
    "##### Target depth is the thresold for the NPQ correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLDとその0.9倍の深度を格納する配列を作成\n",
    "MLD_array = np.full(concat_stheta_array.shape[0], np.nan)  # MLDの結果\n",
    "mld09_idx = np.full(concat_stheta_array.shape[0], np.nan)  # MLDの0.9倍の深度の結果\n",
    "\n",
    "# MLDと、その0.9倍の深度を求める\n",
    "for t in range(concat_stheta_array.shape[0]):\n",
    "    density_profile = lerp_ddata[t, :]  # 時刻 t の密度プロファイル\n",
    "\n",
    "    # 1. 10dbar での密度を取得\n",
    "    rho_10 = density_profile[10]\n",
    "\n",
    "    # 2. MLDを決定（ρ_{10} + 0.03 を初めて超えた深度）\n",
    "    threshold = rho_10 + 0.03\n",
    "    MLD_index = np.argmax(density_profile > threshold)  # 最初に超えたインデックス\n",
    "\n",
    "    if MLD_index == 0 and density_profile[0] <= threshold:\n",
    "        continue  # MLDが見つからなかった場合、np.nan のまま\n",
    "\n",
    "    MLD_depth = depth_lerp[MLD_index]  # MLD の深度\n",
    "    target_depth_value = MLD_depth * 0.9  # 0.9倍の深度\n",
    "\n",
    "    # 3. MLD * 0.9 に最も近い深度を求める\n",
    "    closest_depth_index = np.argmin(np.abs(depth_lerp - target_depth_value))\n",
    "    closest_depth = closest_depth_index\n",
    "\n",
    "    # 結果を保存\n",
    "    MLD_array[t] = MLD_depth\n",
    "\n",
    "    if angle_array[t] <= 0:\n",
    "        continue  # angle_array <= 0 の場合は np.nan のまま\n",
    "    if np.any(concat_qcarray[t, :] == '5'):\n",
    "        continue\n",
    "\n",
    "    mld09_idx[t] = closest_depth\n",
    "\n",
    "# 4. 結果を出力\n",
    "for t in range(lerp_ddata.shape[0]):\n",
    "    print(f\"MLD = {MLD_array[t]} dbar, Target Depth = {mld09_idx[t]} dbar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### NPQ correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 補正後のデータを格納する配列（コピーを作成）\n",
    "chla_lerp_smqc_npq = np.copy(lerp_v2data_adjusted_smqc)\n",
    "\n",
    "# 各時間ステップごとに処理\n",
    "for i in range(lerp_v2data_adjusted_smqc.shape[0]):\n",
    "    idx = mld09_idx[i]  # その時刻の MLD インデックス\n",
    "\n",
    "    # NaN ならスキップ\n",
    "    if np.isnan(idx) or idx >= lerp_v2data_adjusted_smqc.shape[1]:\n",
    "        continue  # MLDインデックスが範囲外ならスキップ\n",
    "\n",
    "    idx = int(idx)  # インデックスを整数化\n",
    "\n",
    "    # 1️⃣ MLD より浅い範囲の最大値を取得\n",
    "    max_chl = np.nanmax(lerp_v2data_adjusted_smqc[i, :idx+1])  # MLDまでの最大値\n",
    "\n",
    "    # 2️⃣ 最大値を 0dbar まで外挿\n",
    "    chla_lerp_smqc_npq[i, :idx+1] = max_chl  # 0dbar から MLD までを最大値で統一"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "#### Dark correction\n",
    "##### We define dark bias from the mode of values in the 300-500 dbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_dark = np.full(chla_lerp_smqc_npq.shape[0], np.nan)\n",
    "\n",
    "for t in range(chla_lerp_smqc_npq.shape[0]):\n",
    "    md = statistics.mode(chla_lerp_smqc_npq[t, 301:501])\n",
    "    print(\"dark bias is \",md)\n",
    "    md_dark[t] = md\n",
    "\n",
    "chla_lerp_smqc_npq_dk = chla_lerp_smqc_npq - md_dark[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "#### Extract chla maxima in subsurface layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#亜表層極大の定義\n",
    "chlmx = np.full([2, chla_lerp_smqc_npq.shape[0]], np.nan)\n",
    "for t in range(chla_lerp_smqc_npq_dk.shape[0]):\n",
    "    chlmx[0, t] = np.nanmax(chla_lerp_smqc_npq_dk[t, :])\n",
    "    if MLD_array[t] > depth_lerp[np.nanargmax(chla_lerp_smqc_npq_dk[t, :])]:\n",
    "        continue\n",
    "    chlmx[1, t] = depth_lerp[np.nanargmax(chla_lerp_smqc_npq_dk[t, :])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Drawing the timeseries of the profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_var = chla_lerp_smqc_npq_dk.transpose(1, 0)\n",
    "    \n",
    "vmin = 0\n",
    "vmax = 0.6\n",
    "vint = 0.05\n",
    "    \n",
    "cm = plt.get_cmap('Greens') \n",
    "    \n",
    "rcParams['figure.figsize'] = 8,2\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "cs = plt.contourf(time_list, depth_lerp, draw_var, \n",
    "                  cmap=cm, norm=Normalize(vmin=vmin, vmax=vmax),\n",
    "                  levels=np.arange(vmin,vmax+vint,vint), extend='both')\n",
    "\n",
    "dlon,ddep = 1,50\n",
    "yticks=np.arange(0, 300,ddep)\n",
    "ax1.set_title(f\"{fnum}\", fontsize=18)\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax1.xaxis.set_major_locator(mdates.MonthLocator(interval = 1))\n",
    "ax1.tick_params(axis='x', rotation=45)  # 日付ラベルを45度傾けて見やすくする\n",
    "ax1.set_yticks(yticks)\n",
    "ax1.axes.tick_params(labelsize=12)\n",
    "ax1.set_ylabel(\"Depth (dbar)\", size=12)\n",
    "ax1.grid(linewidth=1, linestyle=':', color='k', alpha=0.8)\n",
    "ax1.minorticks_on()\n",
    "ax1.invert_yaxis()  # 深度を下向きにする\n",
    "ax1.set_ylim(200, 0)\n",
    "    \n",
    "cbar = plt.colorbar(cs, shrink=0.9) \n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "cbar.set_ticks(np.linspace(vmin, vmax, num=7))\n",
    "cbar.set_label('mg/m$^3$', size=12)\n",
    "\n",
    "# MLDを赤線でプロット\n",
    "ax1.scatter(time_list, MLD_array, c='k',marker='_', label='MLD')\n",
    "ax1.scatter(time_list, chlmx[1], c='r',marker='_', label='Max')\n",
    "# 凡例を追加\n",
    "ax1.legend(loc='lower right', fontsize=12, ncol = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "#### Drawing float trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.ticker import LatitudeFormatter,LongitudeFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "rcParams['figure.figsize'] = 6,6\n",
    "c_lon = 180\n",
    "ax = fig.add_subplot(1,1,1, projection=ccrs.PlateCarree())\n",
    "\n",
    "ax.plot(lon_array[:, 0],lat_array[:, 0], color = 'b', lw = '1.5', transform=ccrs.PlateCarree())\n",
    "\n",
    "ax.plot(lon_array[0],lat_array[0], 'ob', markersize = '8', label = 'start', transform=ccrs.PlateCarree())\n",
    "ax.plot(lon_array[32],lat_array[32], 'xb', markersize = '8', label = 'end', transform=ccrs.PlateCarree())\n",
    "    \n",
    "\n",
    "dlon,dlat=2,2\n",
    "xticks=np.arange(60,360,dlon)\n",
    "yticks=np.arange(-60,70.1,dlat)\n",
    "ax.set_title(f\"{fnum}\", fontsize=18)\n",
    "ax.set_xticks(xticks,crs=ccrs.PlateCarree())\n",
    "ax.set_yticks(yticks,crs=ccrs.PlateCarree())\n",
    "ax.coastlines(lw=1.0,color='black',resolution='50m')\n",
    "\n",
    "latfmt=LatitudeFormatter()\n",
    "lonfmt=LongitudeFormatter(zero_direction_label=True)\n",
    "ax.xaxis.set_major_formatter(lonfmt)\n",
    "ax.yaxis.set_major_formatter(latfmt)\n",
    "ax.axes.tick_params(labelsize=15)\n",
    "ax.legend(fontsize=12, loc = 'lower right')\n",
    "\n",
    "wlim = int(np.round(lon_array.min()-5))\n",
    "elim = int(np.round(lon_array.max()+5))\n",
    "slim = int(np.round(lat_array.min()-5))\n",
    "nlim = int(np.round(lat_array.max()+5))\n",
    "\n",
    "ax.set_xlim(wlim, elim)\n",
    "ax.set_ylim(slim, nlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
