{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Load the libraries (if error occurs, install them via `pip install xarray` or `conda install xarray` for example for `xarray`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage import generic_filter\n",
    "import pvlib\n",
    "import gsw\n",
    "import scipy\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Specify the float number you want to post-process\n",
    "- note the file list is not in sequence. this is intentional to prevent the assumption that the file order follows the time sequence (which is not necessary always the case for some reason)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wmoid = 6901474 # float number\n",
    "vernum = 0 # Version of the notebook (generally, no need to change) \n",
    "\n",
    "list_file = glob.glob(f\"{wmoid}/S*{wmoid}_[0-9][0-9][0-9].nc\")\n",
    "print('Total number of profies:',len(list_file))\n",
    "list_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Read all profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for storing all profiles\n",
    "juld = [] # date and time\n",
    "lon = [] # longitude\n",
    "lat = [] # latitude\n",
    "pres = [] # pressure level (depth)\n",
    "vars = ['temp','psal','down','nitr','chla','bbp7','doxy','ph_i'] # shortened names\n",
    "vars_original = ['TEMP','PSAL', # original names\n",
    "                 'DOWNWELLING_PAR_ADJUSTED','NITRATE_ADJUSTED',\n",
    "                 'CHLA_ADJUSTED','BBP700_ADJUSTED',\n",
    "                 'DOXY_ADJUSTED','PH_IN_SITU_TOTAL_ADJUSTED']\n",
    "raw = {f\"{var}\": [] for var in vars} # raw data\n",
    "qc = {f\"{var}\": [] for var in vars} # qc flags\n",
    "qc_valid = {f\"{var}\": [] for var in vars} # good data\n",
    "pres_qc_valid = {f\"{var}\": [] for var in vars} # corresponding pres for good data\n",
    "npq5_qc_valid = [] # corresponding qc=5 for good chla data (used for identifying whether npq correction is needed)\n",
    "\n",
    "for i in range(len(list_file)): # loop over profiles\n",
    "    ds = xr.open_dataset(list_file[i]) # open the netCDF file for each profile\n",
    "    juld.append(ds['JULD'][0].values)\n",
    "    lon.append(ds['LONGITUDE'][0].values)\n",
    "    lat.append(ds['LATITUDE'][0].values)\n",
    "    pres.append(ds['PRES'][0,:].values)\n",
    "    for j in range(len(vars)): # loop over variables\n",
    "        if vars_original[j] in ds.data_vars:\n",
    "            raw[vars[j]].append(ds[vars_original[j]][0,:].values) # store raw profiles\n",
    "            qc[vars[j]].append(ds[vars_original[j]+'_QC'][0,:].values.astype(str)) # store qc flags\n",
    "            if vars_original[j] == 'CHLA_ADJUSTED': # include the qc flag of 5 (NPQ)\n",
    "                qc_valid[vars[j]].append(raw[vars[j]][-1][np.isin(qc[vars[j]][-1],['1','2','5','8'])]) # store qc masks\n",
    "                pres_qc_valid[vars[j]].append(pres[-1][np.isin(qc[vars[j]][-1],['1','2','5','8'])])\n",
    "                npq5_qc_valid.append(np.any(np.isin(qc[vars[j]][-1],['5']))) # set to True if the profile contains QC of 5 (NPQ corrected)\n",
    "            else:\n",
    "                qc_valid[vars[j]].append(raw[vars[j]][-1][np.isin(qc[vars[j]][-1],['1','2','8'])]) # store qc masks\n",
    "                pres_qc_valid[vars[j]].append(pres[-1][np.isin(qc[vars[j]][-1],['1','2','8'])]) # store qc masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Plot the raw data to understand the data coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raw(vari_in,pres_in,juld_in,vari_name_in):\n",
    "    for i in range(len(vari_in)): # loop over profiles\n",
    "        if np.any(np.isfinite(vari_in[i])): # ignore the profile with all NaNs\n",
    "            plt.scatter(np.full(len(pres_in[i]),juld_in[i]),pres_in[i],c=vari_in[i],s=0.1,\n",
    "                        vmin=np.nanmin(np.concatenate(vari_in)),\n",
    "                        vmax=np.nanmax(np.concatenate(vari_in))\n",
    "                       )\n",
    "    plt.gca().invert_yaxis()\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label(vari_name_in)\n",
    "    vari_ptile = [np.nanpercentile(np.concatenate(vari_in),25),\n",
    "                  np.nanpercentile(np.concatenate(vari_in),50),\n",
    "                  np.nanpercentile(np.concatenate(vari_in),75)\n",
    "                 ]\n",
    "    cbar.ax.hlines(vari_ptile,xmin=0,xmax=1,color='r') # draw percentiles\n",
    "    plt.gcf().autofmt_xdate() # automatically format date\n",
    "    plt.xlim(np.min(juld),np.max(juld)) # align the date range across all variables\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "for j in range(len(vars)): # loop over variables\n",
    "    plt.subplot(3,3,j+1)\n",
    "    if len(raw[vars[j]]) and np.any(np.isfinite(np.concatenate(raw[vars[j]]))): # if finite values exit\n",
    "        plot_raw(raw[vars[j]],pres,juld,vars_original[j])\n",
    "    else:\n",
    "        plt.text(0.1,0.5,'NO DATA for \\n'+vars_original[j])\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Plotting the good data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "for j in range(len(vars)): # loop over variables\n",
    "    plt.subplot(3,3,j+1)\n",
    "    if len(qc_valid[vars[j]]) and np.any(np.isfinite(np.concatenate(qc_valid[vars[j]]))): # if finite values exit\n",
    "        plot_raw(qc_valid[vars[j]],pres_qc_valid[vars[j]],juld,vars_original[j])\n",
    "    else:\n",
    "        plt.text(0.1,0.5,'NO DATA for \\n'+vars_original[j])\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN を無視する中央値フィルタ関数\n",
    "def nanmedian_filter(values):\n",
    "    valid_values = values[~np.isnan(values)]  # NaN を除去\n",
    "    return np.median(valid_values) if len(valid_values) > 0 else np.nan  # 有効値があれば中央値、なければ NaN\n",
    "\n",
    "# define lists\n",
    "smooth = {f\"{var}\": [] for var in vars} # smoothed data\n",
    "pres_res = {f\"{var}\": [] for var in vars} # vertical resolution\n",
    "pres_mid = {f\"{var}\": [] for var in vars} # midpoint depth at which vertical resolutions are defined\n",
    "\n",
    "# Compute and assign smoothed values\n",
    "for j in range(len(vars)): # loop over variables\n",
    "    for i in range(len(qc_valid[vars[j]])): # loop over profiles\n",
    "        pres_res[vars[j]].append(np.diff(pres_qc_valid[vars[j]][i])) # 深度の解像度を計算\n",
    "        pres_mid[vars[j]].append((pres_qc_valid[vars[j]][i][:-1] + pres_qc_valid[vars[j]][i][1:]) / 2) # 深度の解像度を計算\n",
    "        pres_res_med = np.median(pres_res[vars[j]][-1])\n",
    "        # 窓サイズの決定\n",
    "        if pres_res_med >= 3:\n",
    "            nsmooth = 5\n",
    "        elif pres_res_med <= 1:\n",
    "            nsmooth = 11\n",
    "        else:\n",
    "            nsmooth = 7\n",
    "        # 中央値フィルタを適用\n",
    "        smooth[vars[j]].append(generic_filter(\n",
    "            qc_valid[vars[j]][i],nanmedian_filter,size=nsmooth,mode='nearest')\n",
    "                              )\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,10))\n",
    "for j in range(len(vars)): # loop over variables\n",
    "    plt.subplot(3,3,j+1)\n",
    "    if len(smooth[vars[j]]) and np.any(np.isfinite(np.concatenate(smooth[vars[j]]))): # if finite values exit\n",
    "        plot_raw(smooth[vars[j]],pres_qc_valid[vars[j]],juld,vars_original[j])\n",
    "    else:\n",
    "        plt.text(0.1,0.5,'NO DATA for \\n'+vars_original[j])\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Plot the vertical resolution information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_res(pres_res_in,pres_mid_in,vari_name_in):\n",
    "    for i in range(len(pres_res_in)):\n",
    "        if np.any(np.isfinite(pres_res_in[i])): # ignore the profile with all NaNs\n",
    "            plt.scatter(pres_res_in[i],pres_mid_in[i],color='k',s=0.1,alpha=0.1)\n",
    "    plt.title(vari_name_in)\n",
    "    plt.xlim(0,np.nanmax(np.concatenate(pres_res_in)))\n",
    "    plt.ylim(0,np.nanmax(np.concatenate(pres_mid_in)))\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('Resolution (dbar)')\n",
    "    plt.ylabel('Depth (dbar)')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,10))\n",
    "for j in range(len(vars)): # loop over variables\n",
    "    plt.subplot(3,3,j+1)\n",
    "    if len(pres_res[vars[j]]) and np.any(np.isfinite(np.concatenate(pres_res[vars[j]]))): # if finite values exit\n",
    "        plot_res(pres_res[vars[j]],pres_mid[vars[j]],vars_original[j])\n",
    "    else:\n",
    "        plt.text(0.1,0.5,'NO DATA for \\n'+vars_original[j])\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Interpolation (Please specify the resolution and the depth for interpolation)\n",
    "- date is sorted for 2d interpolation\n",
    "- Set negatives to zeros for all variables other than temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_res = 10 # resolution used for interpolation (dbar, which can be approximated as meter)\n",
    "int_dep0 = 1 # the shallowest depth\n",
    "int_dep1 = 1000 # the deepest depth\n",
    "\n",
    "pres_int = np.arange(int_dep0,int_dep1,int_res) # depth grid for interpolation\n",
    "data_int = {f\"{var}\": [] for var in vars} # interpolated data\n",
    "\n",
    "def interpolate_argo(pres_in,pres_out,data_in,date,vari_name_in):\n",
    "    data2d = np.full((len(date),len(pres_out)), np.nan) # create 2d array filled with NaNs\n",
    "    ind_sorted = np.argsort(date)\n",
    "    for i in range(len(pres_in)):\n",
    "        if np.any(np.isfinite(pres_in[ind_sorted[i]])): # ignore the profile with all NaNs\n",
    "            data_sorted = data_in[np.argsort(date)[i]]\n",
    "            f = scipy.interpolate.interp1d(x=pres_in[ind_sorted[i]],y=data_in[np.argsort(date)[i]],\n",
    "                                           kind='linear',\n",
    "                                           bounds_error=False,  # 範囲外は補間せずにfill_valueを適用\n",
    "                                           fill_value=np.nan    # 範囲外のデータはNaNに設定\n",
    "                                          )\n",
    "            data2d[i,:] = f(pres_out)\n",
    "    return np.float32(data2d) # single precision is sufficient\n",
    "\n",
    "# function to plot the interpolated data (2d array)\n",
    "def plot_int(vari_in,pres_in,juld_in,vari_name_in):\n",
    "    X, Y = np.meshgrid(np.sort(juld_in),pres_in, indexing='ij')  # (time, depth)\n",
    "    #X, Y = np.meshgrid(np.sort(juld_in),pres_in)  # shape must match int_2d\n",
    "    if np.any(np.isfinite(vari_in)): # check at least one value exits\n",
    "        plt.scatter(X,Y,c=vari_in,s=0.1,vmin=np.nanmin(vari_in),vmax=np.nanmax(vari_in))\n",
    "    else: # all values are NaNs, which seems weird\n",
    "        print('all interpolated values are NaN? CHECK')\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label(vari_name_in)\n",
    "    vari_ptile = [np.nanpercentile(vari_in,25),\n",
    "                  np.nanpercentile(vari_in,50),\n",
    "                  np.nanpercentile(vari_in,75)\n",
    "                 ]\n",
    "    cbar.ax.hlines(vari_ptile,xmin=0,xmax=1,color='r') # draw percentiles\n",
    "    plt.gcf().autofmt_xdate() # automatically format date\n",
    "    plt.xlim(np.min(juld),np.max(juld)) # align the date range across all variables\n",
    "    plt.ylim(0,np.max(pres_in)) # align the depth range across all variables\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "# Interpolate and plot\n",
    "plt.figure(figsize=(12,10))\n",
    "for j in range(len(vars)): # loop over variables\n",
    "    plt.subplot(3,3,j+1)\n",
    "    if len(smooth[vars[j]]) and np.any(np.isfinite(np.concatenate(smooth[vars[j]]))): # if finite values exit\n",
    "        data_int[vars[j]] = interpolate_argo(pres_qc_valid[vars[j]],pres_int,smooth[vars[j]],juld,vars_original[j])\n",
    "        if vars[j] != 'temp': # if not temperature\n",
    "            data_int[vars[j]][data_int[vars[j]]<0] = 0 # set negatives to zeros\n",
    "        plot_int(data_int[vars[j]],pres_int,juld,vars_original[j])        \n",
    "    else:\n",
    "        plt.text(0.1,0.5,'NO DATA for \\n'+vars_original[j])\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Saving\n",
    "Save the post-processed 2D array as a netCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary to hold DataArrays\n",
    "data_vars = {}\n",
    "\n",
    "# Loop to generate DataArrays\n",
    "for i in range(len(vars)):\n",
    "    if len(data_int[vars[i]]): # continue if data exist\n",
    "        data_array = xr.DataArray(\n",
    "            data_int[vars[i]],\n",
    "            coords={\n",
    "                'time': ('time', np.sort(juld)), #, {'units': 'days since 1950-01-01'}),\n",
    "                'depth': ('depth', pres_int, {'units': 'dbar'})  # or 'meters' if it's depth below sea surface\n",
    "            },            \n",
    "            dims=['time', 'depth'],\n",
    "            attrs=ds[vars_original[i]].attrs # copy the input file attributes\n",
    "        )\n",
    "        data_vars[vars_original[i]+'_AR'] = data_array # adding the array to the dataset (AR: Analysis-Ready)\n",
    "    else: # skip if data are empty\n",
    "        print(vars_original[i],'is empty so not adding to the file')\n",
    "\n",
    "# Create Dataset from all variables\n",
    "ds_int = xr.Dataset(data_vars)\n",
    "\n",
    "print(ds_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## EXTRA (variable-specific post-processing)\n",
    "- Chlorophyll-a: NPQ and dark corrections\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### **Chlorophyll-a only** Apply NPQ correction and dark correction for relevant profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_solar_elevation(latitude, longitude, utc):\n",
    "    \"\"\"緯度, 経度, UTC時間 から太陽光角度を計算\"\"\"\n",
    "    loc = pvlib.location.Location(latitude, longitude)\n",
    "    solar_position = loc.get_solarposition(utc)\n",
    "    solar_zenith = solar_position['zenith'].values[0]  # 太陽天頂角\n",
    "    solar_elevation = 90 - solar_zenith  # 太陽高度角\n",
    "    return solar_elevation\n",
    "\n",
    "# Define empty lists\n",
    "mld = [] # mixed layer depth\n",
    "chla_npq = [] # NPQ corrected\n",
    "chla_dark = [] # dark corrected\n",
    "count_npq = 0 # number of NPQ correction applied here\n",
    "\n",
    "# Compute and assign NPQ corrected values\n",
    "if len(qc_valid['chla']): # if chla data exist:\n",
    "    for i in range(len(qc_valid['chla'])): # loop over profiles        \n",
    "        if calc_solar_elevation(lat[i], lon[i], juld[i]) < 0 and not npq5_qc_valid[i]: # if sun is above horizon AND qc != 5 (NPQ correction necessary)\n",
    "            # 絶対塩分（Absolute Salinity, SA）の計算\n",
    "            SA = gsw.SA_from_SP(smooth['psal'][i], pres_qc_valid['psal'][i], lon[i], lat[i])        \n",
    "            # 実効温度（Conservative Temperature, CT）の計算\n",
    "            CT = gsw.CT_from_t(SA, smooth['temp'][i], pres_qc_valid['temp'][i])\n",
    "            # ポテンシャル密度（Potential Density, σθ）の計算（基準圧力 0 dbar）\n",
    "            sigma0 = gsw.sigma0(SA, CT)  # σθ = 密度 - 1000 (kg/m³)\n",
    "            # Obtain sigma0 at 10 dbar based on linear interpolation 10dbarでの密度を取得\n",
    "            sigma0_10 = np.interp(x=pres_qc_valid[i],xp=10,fp=sigma0)\n",
    "            for j in range(len(sigma0)): # loop over samples\n",
    "                if sigma0[j] > sigma0_10 + 0.03:\n",
    "                    mld.append(pres_qc_valid['temp'][i][j])\n",
    "                    idx90 = np.argmin(np.abs(pres_qc_valid['chla'][i] - 0.9*mld[-1])) # depth index closest to mld*0.9\n",
    "                    chla_npq.append(smooth['chla'][i]) # first assign the smoothed data\n",
    "                    chla_npq[-1][:idx90+1] = np.nanmax(chla_npq[-1][:idx90+1]) # set the upper 90% mld to have uniform chla\n",
    "                    count_npq += 1 # count the number of corrected profiles\n",
    "                    break # stop at the first occurrence\n",
    "            else: # if the threshold is never met\n",
    "                mld.append(np.nan) # assign nan\n",
    "                chla_npq.append(smooth['chla'][i]) # no NPQ correction hence assign the smoothed data\n",
    "        else:\n",
    "            chla_npq.append(smooth['chla'][i]) # assign the smoothed data\n",
    "    else:\n",
    "        print ('No good data for CHLA, so not calculating NPQ or dark correction')\n",
    "\n",
    "print('Total number of all profiles:',len(npq5_qc_valid))\n",
    "print('Total number of profiles with QC = 5:',sum(npq5_qc_valid))\n",
    "print('Total number of profiles corrected here:',count_npq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Saving to netCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_int.attrs['title'] = 'Analysis-ready BGC-Argo dataset'\n",
    "ds_int.attrs['institution'] = 'JAMSTEC Application Laboratory (APL)'\n",
    "ds_int.attrs['notes'] = 'Reference: Hayashida and Fujishima (2025): Jupyter Notebook for generating analysis-ready BGC-Argo datasets, Journal of Open Source Software'\n",
    "ds_int.attrs['history'] = 'Created on ' \n",
    "                        + datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "                        + ' using Version '+str(vernum)\n",
    "# Save to NetCDF\n",
    "ds_int.to_netcdf('AR'+str(wmoid)+'.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
